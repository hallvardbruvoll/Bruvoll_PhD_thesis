# Methods: Distribution fitting {#methods-distfit}

```{r 05-libraries, include=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
```

In this chapter I go into some more detail around the methods used in Chapter \@ref(results-distfit), and the reasoning underlying my choices of methods. As mentioned earlier (Sections \@ref(research-question-and-objectives) and \@ref(power-law)), I consider power-law distributions as a statistical signature of hierarchical structures, and wish to test whether such structures may be reasonably shown to exist among houses in European Neolithic villages, or if house sizes in these contexts are better explained by other non-hierarchical models. Results from these analyses add to current debates surrounding the development of social and political organisation in the Neolithic, and to the question of the emergence of stratified societies more broadly. Furthermore, and as also mentioned in Section \@ref(power-law), the methodological procedure leading to claims of power-law distributed data is not entirely straightforward, and is an issue that has undergone important developments in recent years, often leading to refutations of earlier claims. It is therefore critical to be explicit as to the methods being applied in studies like this one, and not simply report results obtained in some unspecified way through obscurely documented software.

In the following I will present the methodological procedure applied in the distribution fitting on house sizes done in Chapter \@ref(results-distfit), followed by a series of tests of this procedure on synthetically generated data, with the goal of obtaining a more detailed view of the accuracy and limits of the method. Due to limited space and for simplicity, I will concentrate on the choices of methods and procedure, and not on the under-the-hood functioning of different statistical tools like maximum likelihood estimation and calculation of the Akaike information criterion. For more details on these there is a number of good introductory volumes, some of which -- like @shennan2008 and @baxter2003 -- are also specifically aimed at archaeologists.

## Modelling heavy-tailed distributions

The standard method for fitting power-law models to empirical data throughout the 20^th^ century was through least squares linear regression on log-transformed x and y values [e.g. @harrison1981; @mitzenmacher2004], the same way exponential and log-normal models could be fit more easily to data by log-transforming x values. Conscious about the still frequent lack of statistical training among archaeologists, @brown2005 and @brown2010 presented the log-linear regression method as sufficient because of its simplicity of application compared to more sophisticated methods. @brown2010 furthermore provided a detailed discussion around how to plot the data in order to obtain the most accurate parameter estimates. The central problem with fitting power-law models to data, is that power-law distributions characteristically have an overwhelmingly large proportion of the data at lower values, while the scarce high values are typically several orders of magnitude higher. Density plots of empirical data (with the PDF on the y axis) require binning, so that the plotted data points in reality correspond to bar heights in a histogram. The applied bin width will furthermore have a heavy influence on the appearance of the plot, where small bin widths are best to represent the many low values and large bin widths are best for the few high values. Brown and Liebovitch proposed multi-scale PDFs, combining histograms of different bin widths before performing regression, which they showed give better results on synthetic data [-@brown2010, Chapter 2]. Logarithmic binning -- increasing bin width exponentially so that points appear to be spaced as constant increments on a log-transformed x axis is also a possibility that has been proposed [@newman2005, pp. 325-6]. Plotting the cCDF instead of the PDF has the advantage of avoiding the bin-width issue altogether, since y values then are a function of the rank of each data point. This also allows for using all the data and not reducing it into bins, and is shown on synthetic data to give more accurate $\alpha$ estimates (i.e. absolute slope of cCDF $+ 1$, see Eq. \@ref(eq:zipf-exponent)). However, the inconvenience with fitting the power-law model to the cCDF, as pointed out by @brown2010, is that it does not necessarily form a straight line, but will in particular be curved when $\alpha \leq 1$, making it more difficult to distinguish visually from other heavy-tailed distributions.

From the early 2000s, physicists and mathematicians started to criticise the frequent use of log-linear regression methods for modelling power laws, since they were shown to introduce systematic biases to the parameter estimates no matter the adopted plotting method, and calls for the use of more robust methods like maximum likelihood estimation (MLE) were put forth [e.g. @newman2005, pp. 325-7; @stumpf2012]. A new methodological tool kit was proposed by @clauset2009, which has since seemingly become the new gold standard for fitting heavy-tailed distributions. One of their main critiques of earlier practices, came from the recognition that in nearly all real-world contexts where power laws are claimed to exist, this behaviour only kicks in from some lower threshold or $x_{min}$ in the terminology of @clauset2009. In earlier studies the value of this threshold was simply set by guessing from the looks of the plot and trying to fit a line covering as much as possible of the data. More formally, this also impeded proper normalising of the distribution. The method proposed by @clauset2009 consisted in testing a range of different $x_{min}$ values and picking the one that gave the best MLE fit to the data by minimising the KS or Kolmogorov-Smirnov statistic (the largest observed distance between the model and the data), and was reported to perform very well on synthetic data. Next they tested the plausibility of the power-law model through bootstrapping, i.e. generating a large number of synthetic random data sets with the same estimated parameter values, each time measuring the KS statistic compared to the ideal model. The fraction of runs giving a KS statistic higher than that of the empirical data gives the *p*-value, which they argued should lead to a rejection of the power-law hypothesis when $p < 0.1$. They estimated the number of bootstrapping runs necessary for robust p-values being between 1.000 and 10.000, which for a few data series is not dramatic, but for larger numbers of data series quickly becomes computationally intensive. But most importantly, they argued that the bulk of previous studies claiming to find power laws in empirical data never actually tested and compared their model with alternative models, which they argued should be done even in convincing cases where a power-law model could not be excluded as a good fit through bootstrapping. The method they proposed for comparison and selection between competing models, was Vuong's log-likelihood pairwise comparison test, though pointing out that any good statistical model selection method could serve this purpose [see @clauset2009, p. 663 for an overview of their "recipe for analysing power-law distributed data"]. These methods were later implemented with functions and documentation in the R package *poweRlaw* [@gillespie2015], which has been used and cited in at least some archaeological studies since [@crabtree2017; @haas2015].

For the present study I have largely chosen to follow the instructions advocated by @clauset2009, but with a few modifications, for reasons that are discussed in more detail below. Early experiments with the *poweRlaw* package indicated that the proposed bootstrapping procedure possibly represented a slight overkill in the present context, requiring much computing time with relatively limited gains. Furthermore, the pairwise model comparison using the Vuong's log-likelihood test appeared good but somewhat tedious, requiring a nested algorithm eliminating competing models one by one. Therefore I decided instead to opt for testing all candidate model simultaneously using the Akaike Information Criterion (AIC), or rather the version of it designed to correct for small sample sizes -- the AICc. The AIC score is also calculated from the log-likelihood of each model, and indicates which of the candidates accounts for the given data with the most weight. While not implemented in the *poweRlaw* package, it is a frequently used statistical tool implemented in a number of accessible R packages. Here I used the *AICcmodavg* package [@AICcmodavg-package] because of its useful functions for manually tackling the differences in how base R and *poweRlaw* models are coded. I also tested using the Bayesian Information Criterion (BIC) on the data sets, but quickly came to the conclusion that the AICc was sufficient for the types of models being used here, with maximum two parameters. The tested model types for the distribution tails were power law, log-normal, exponential and stretched exponential/Weibull, all of which were fitted using the *poweRlaw* package.

Having defined the best possible power-law model to the data series and selected the best candidate model for the distribution tail from the same $x_{min}$ value applying the above procedure, I went on to test for the best model of the whole data series, without setting a lower threshold. This excludes per definition the power-law model, and I also excluded the Weibull distribution since it is more general and can mimic other models without providing very useful theoretical explanations. The Weibull distribution -- much like the gamma distribution -- can be a very handy tool for modelling empirical data when the goal is to use a single model type for data series with multiple types of shape, or for prediction in many practical settings, but it has much more limited explanatory power since it lacks a broad theoretical generative mechanism like the Central Limit Theorem or preferential attachment. The reason for including it when comparing tail models is that it provides a good power-law-like approximation with finite variance, i.e. it allows for upper bounds. The tested models for the whole data range were therefore the normal, log-normal and exponential distributions, all fitted with MLE using the broad and well established *MASS* package [@venables2002], and selected with AICc as with the distribution tails.

## Testing for false positive power-law tails

The difficulty of comparing power-law models with other common candidate models (like log-normal or exponential), is that they, unlike the others, by definition need a specified lower bound above 0, denoted $x_{min}$. Comparison of multiple models with AIC scores is only meaningful when done over the same range of data [this also applies to the Vuong's log-likelihood test for pairs of models proposed by @clauset2009]. However, comparing multiple models over the range in a data set which has already been recognised as providing the best possible fit for a power-law model, gives this latter model a potential advantage over the other ones. Log-normal models, for instance, can explain the entire range of a data distribution, where a power law can in most cases only explain the highest values in the distribution tail. The fact that these two model types have frequently and for a long time represented competing explanations for the same empirical data sets, may reflect this apparent incomparability between them [e.g. @bee2011; @gibrat1930; @harrison1981; @mitzenmacher2004; @sheridan2018]. One can suspect then that this procedure of distribution fitting and model selection would favour power-law models unreasonably. At the same time, one of the main findings of the @clauset2009 study, was that power-law behaviour was only confirmed beyond reasonable doubt in one out of 24 empirical data sets which had been reported as power-law distributed in earlier studies, leaving the impression that the methodology would be conservative rather than lenient. In many cases however, the study remained inconclusive, especially regarding comparisons between power-law and log-normal models to empirical data sets (comparisons between power-law and other models were generally more conclusive). The authors admitted "*In general, we find that it is extremely difficult to tell the difference between log-normal and power-law behaviour. Indeed, over realistic ranges of* x *the two distributions are very close, so it appears unlikely that any test would be able to tell them apart unless we had an extremely large data set*" [@clauset2009, p. 689]. Extremely large data sets are of course a luxury that is rarely afforded in archaeology, and if these two models are that close in many situations, one can ask whether picking one over the other really matters in the end. This question is further developed in Chapter \@ref(disc-methods).

The reliability of this methodology can to some extent be assessed using synthetic data. A first question to address is whether sample size affects the selected distribution model for the tail, and if so what size should be considered a minimum for the results to be reliable. In Figure \@ref(fig:05-distfit), using random number generator functions in base R [@R2023] and with the *poweRlaw* package [@gillespie2015], I reproduced Fig. 5a in @clauset2009, namely examples of a power-law, a log-normal and an exponential distribution, with the addition here of a stretched exponential, illustrating how they all can look roughly linear on log-log plots with their survival functions/cCDFs. Using the same parameter values, but with four different sample sizes (10, 100, 1.000 and 10.000) on each model, these test distributions were run through the distribution fitting algorithm described above (Figs. \@ref(fig:05-tails) and \@ref(fig:05-pltails)). The power law was correctly identified no matter the sample size, but for the smallest sample size ($n = 10$) all other distribution types also gave power-law tails. For $n \geq 1000$ all distribution types were correctly identified also in their tails, while for $n = 100$ this was only the case for the stretched exponential and the power law. These results are in agreement with the analysis based on p-values obtained from bootstrapping presented by Clauset et al. [-@clauset2009, p. 676 ff.], but the method opted for here is far less computationally intensive. Selecting the best model alternative directly based on AICc is also a less complex operation compared to the sequence of first bootstrapping and then performing pairwise comparisons of log-likelihood as proposed by @clauset2009. The inconvenience with the method proposed here, is of course that there is no guarantee that any of the models tested for are in reality appropriate -- we only find out *which* one of them is the *most* appropriate. The p-value approach in [@clauset2009] does allow for positively rejecting hypotheses that clearly do not fit the data. However, early experiences (not presented in further detail here) gave the impression that this made little practical difference, at least on the data sets analysed in this thesis. Most sample sizes are in the order of $100$ or lower, in which cases bootstrapping remained inconclusive, while it was still interesting to have an indication of which model that gave the best fit. The results shown in Figure \@ref(fig:05-tails) indicate that for sample sizes below ca. $100$ power-law interpretations should be treated with care, and should not be trusted as $n$ approaches ca. $10$.

However, it must be noted that in order to generate the non-power-law distributions with a defined lower threshold as done here [and in @clauset2009], a much larger number of data points is in reality needed if we also consider those falling below that same threshold. For example, to get $1000$ data points with values above $x = 15$ following the log-normal distribution shown in Figure \@ref(fig:05-distfit), they need to be filtered out from a total distribution almost ten times larger. When considering entire data distributions without lower bounds, as is usually the case e.g. when analysing archaeological house-size distributions, the sample size will potentially also need to be much larger for correct model selection, although exactly *how* much larger should depend on the type of distribution and parameter values of the data. Similarly, when Clauset et al. argued that with the MLE method for estimating $\alpha$ in a power-law model, sample sizes around $n \geq 50$ would usually be enough for the estimates to be within 1% accurate [-@clauset2009, p. 669], sample size is here referring to the number of data points actually being considered when fitting, which is $n > x_{min}$ only. For this number to be $50$ or higher, the total size of the distribution could often need to be $500$ or higher, which is far more than most of the house counts per village in this study. This matter of sample size is a question that is perhaps less relevant to physicists and mathematicians, but that may be of crucial importance to archaeologists who regularly suffer from limited amounts of data.

(ref:05-distfit) Synthetic data series drawn from four different distribution types: exponential ($\lambda=0.125$), log-normal ($\mu=0.3$, $\sigma=2$), power-law ($\alpha=2.5$) and stretched exponential/Weibull ($shape = 0.5$ and $scale = 3$), all with $n=100$ data points and $x_{min}=15$. Plot equivalent to Fig.5a in @clauset2009, with deviations due to random fluctuations only. Scales are logarithmic, and all four series appear as roughly straight lines, though only one is a true power law

```{r 05-distfit, fig.cap="(ref:05-distfit)"}
load("Results/fig05_synthdist.RData")
fig05_synthdist
```

(ref:05-tails) Selected tail models for the same synthetic data sets, each with four sample sizes ($n=10^1, 10^2, 10^3, 10^4$). For each tail model, $x_{min}$ is set at the value which gives the best power-law fit. Point size indicates fraction of data points thus included in the tail model. For power-law distributions, all samples are correctly identified, while this is the case only for large samples ($n>10^2$) of log-normal and exponential samples, smaller samples being interpreted as having power-law or stretched exponential tails

```{r 05-tails, fig.cap="(ref:05-tails)"}
load("Results/fig05_type_tail.RData")
fig05_type_tail
```

(ref:05-pltails) Boxplot of all the synthetic data sets, overlaid (in red) with the data points interpreted as power-law tails. X axis is logarithmic -- however the log-normal distributions do not appear symmetric since they are truncated with a lower threshold. Note that especially for log-normals and power laws, larger samples give longer tails. If the model predicts a probability of having a value of 10.000 or more as only 1 in 10.000 or 0.01%, a sample size of 10.000 will probably allow for one such value

```{r 05-pltails, fig.cap="(ref:05-pltails)"}
load("Results/fig05_synth_pl.RData")
fig05_synth_pl
```

\FloatBarrier

A second question more specifically related to the selection between log-normal and power-law models, is whether certain parameter combinations increase the likelihood of log-normal distributions being incorrectly interpreted as power laws. In his extensive review of power-law generating mechanisms, Newman [-@newman2005, p. 347-8] showed algebraically how log-normal distributions can be mistaken for power laws especially when the range of the data that is being analysed is short, and when the value of $\sigma$ is high. More specifically, since the PDF of a log-normal on log scales is a quadratic function -- i.e. a parabola -- sufficiently smaller sections of this will be nearly indistinguishable from straight lines, and can thus be well modelled as a power law (Figure \@ref(fig:04-PDF)b). The curvature of the function is characterised by its quadratic term, which is a fraction with $x$ in the numerator and $\sigma$ in the denominator, written $-\frac{(\ln x)^2}{2\sigma^2}$, essentially causing a flatter curvature with higher values of $\sigma$ since this term then will vary more slowly with $x$ [see Eq. 84 in @newman2005 for more details]. The difficulty of distinguishing the two model types empirically undoubtedly lies in the close relationship between them, both being defined as some enhanced exponential distribution [@mitzenmacher2004]. Figure \@ref(fig:05-synth-ln) shows the cCDF plot of 36 synthetically constructed log-normal distributions, with $\mu$ values ranging from 1 to 6 in integer increments, and $\sigma$ values from 0.5 to 3 in increments of 0.5, each with sample size $n = 1000$ and no truncation (i.e. $x > 0$). When plotted this way (with the survival function of the variable), low $\sigma$ values generate angular curves, while high $\sigma$ values generate more parabolic curves.

(ref:05-synth-ln) cCDF plot of 36 synthetic log-normal distributions with parameter values $1 \leq \mu \leq 6$ and $0.5 \leq \sigma \leq 3$. Each distribution is generated with $n = 1000$ data points, but rendered here as lines for clarity. Scales are logarithmic

```{r 05-synth-ln, fig.cap="(ref:05-synth-ln)"}
load("Results/fig05_synth_ln.RData")
fig05_synth_ln
load("Results/pretest2_summary.RData") 
pretest2_summary <- filter(pretest2_summary, tail == "pl") %>%    
  mutate(ntail_n = ntail/n)
```

Running these distributions through the distribution-fitting and model-selecting algorithm, more than half of them are interpreted with a power-law tail (19 of 36, Figures \@ref(fig:05-ln-tail) and @ref(fig:05-ln-pl)). These are seemingly spread throughout the parameter space, with the only clear pattern being that for the highest $\sigma$ values, the power-law tails cover only smaller fractions of the data.

(ref:05-ln-tail) Interpreted tail models of the same log-normal distributions. 19 of 36 distributions have tails that are best modelled as power laws. Symbol size indicates fraction of the data included in the tail, with $x_{min}$ parameter set for best possible power law fit. See text for details

```{r 05-ln-tail, fig.cap="(ref:05-ln-tail)"}
load("Results/fig05_ln_tail.RData")
fig05_ln_tail
```

(ref:05-ln-pl) Boxplot of the same 36 synthetic log-normal distributions, overlaid (in red) with data points included in tails interpreted as power laws. The power-law tails stretch across the log-normal data in a range from `r round(min(pretest2_summary$ntail_n)*100, 3)`% (3 data points out of 1000) to `r round(max(pretest2_summary$ntail_n)*100, 3)`%

```{r 05-ln-pl, fig.cap="(ref:05-ln-pl)"}
load("Results/fig05_ln_pl.RData")
fig05_ln_pl
```

What are we then to conclude from these preliminary tests? Clearly, it is a challenge to confidently distinguish between log-normal and power-law distributed data in the high ends of distributions, as noted in the beginning of this section. If the proposed methodological procedure seemingly serves well to identify power-law distributions when that is what they really are (i.e. there are no false negatives), it also seemingly identifies these erroneously in the tails of log-normal distributions half of the time, irrespectively of the log-normal parameter values, and with (for archaeologists) optimistic sample sizes. A more positive way to look at this issue, is to acknowledge that in truly log-normally distributed data, some definable portion of the upper tail is in many cases indistinguishable from a power law, and actually best modelled as such. There may well be precise mathematical reasons behind this, but further insight to whether such power-law tails are confidently indicative of social hierarchies when observed on material culture proxies such as house sizes, would perhaps require large-scale systematic testing on ethnographically documented cases, which would clearly go beyond the scope of this thesis.

\FloatBarrier

## False positives from data aggregation

Data aggregation or lumping of samples may be done for two main reasons in archaeology. Firstly, when the sample size is too small lumping together several contemporary samples can raise the sample size to an acceptable level (spatial lumping). And secondly, in cases when it is impossible to temporally disentangle elements within a settlement, i.e. to reduce temporal resolution to coeval elements, we are forced to proceed with temporal lumping [accepting a low temporal resolution, @perreault2019, pp. 56-61 ff.]. The problem can be further broken down to two case types: a) all lumped samples are really drawn from the same underlying distribution (referred to as the i.i.d. condition in the previous chapter), and b) they are not similarly distributed. For spatial lumping this degree of similarity can be assessed, but not necessarily for temporal lumping. But even when it can be assessed, the lumping needs to be justified in social terms. As an example, several spatial samples (e.g. villages in a region) can have house-size distributions in which no significant differences are observed using statistical tests like ANOVA, but at the same time be functionally entirely independent, in which case it is arguably more logical to augment the size of a single sample through simulation and evaluate plausibility through bootstrapping, rather than by lumping of all available samples. On the other hand, house sizes can be significantly different between quarters or suburbs within a city or metropolitan area, or even between cities in a region or country, but if they all function together in a coherent system, they may reflect a spatial segregation between different strata in the society, in which case it can make much sense to lump and analyse them together. When it comes to temporal lumping, given that a settlement does not undergo substantial cultural changes during its timespan (change in archaeological culture), a workaround to evaluate whether the house-size distribution evolves significantly over time may be to target a number of size categories for ^14^C dating, and to check that they all stretch over the entire range of the settlement's duration, and if yes, accept to analyse the whole distribution as one.

In order to build an appreciation of the possible effects of data aggregation on the identification of power laws, I constructed a set of 20 synthetic data series in successive steps. The first series consisted of 100 log-normally distributed random numbers with $\mu = 4.5$ and $\sigma = 0.4$, corresponding to sizes of ca. 90 for the mean and 1.5 for the standard deviation when exponentiated. These values were set to be close to realistic values of house-size distributions in Neolithic settlement as presented in the next chapter. The second data series consisted of the previous plus an additional 100 random numbers with the same parameter values, and so on for 20 iterations, so that the last series consisted of 2000 points. These series were then run through the distribution fitting algorithm for best power-law fits and model selection with AICc (Figures \@ref(fig:05-multi-ln)a and \@ref(fig:05-multi-ln-box)a). Five of the 20 series were interpreted with power-law tails (1/4), and these were clustered in two groups. The example is only illustrative, and would need larger and more systematic analyses to be considered general, but these results seem to indicate that under these conditions (increasing sample size with identically distributed, though not independent samples) power-law tails appear somewhat randomly, not necessarily as a result of larger or smaller sample sizes. However, once a power-law tail has appeared, it lingers for one or two iterations since the following series are only copies of the previous with some additional data.

It is important to note here that this additive process is not equal to simply constructing random log-normal sequences with gradually increasing sample size, as was done above (Figures \@ref(fig:05-tails) and \@ref(fig:05-pltails)). There it was shown that for log-normal and power-law distributions, data range increases with sample size since larger samples allow for data points with values that have lower probability of occurring. The data series presented here have internally very similar ranges of x values, since for every iteration new data is added *as if* the distribution only had the original sample size of 100, which is closer to what actually happens when we mix together analytically different phases of a settlement. A settlement with a long duration like 20 generations can thus have a house-size distribution that easily appears as a log-normal distribution with an upper truncation when all houses are analysed together. Though this point is not pursued further here, such truncations could then in themselves be seen as indications of temporal depth in a settlement -- i.e. if the modelled house-size distribution would be expected, given the sample size, to yield some fraction of data points with markedly higher values than what is observed, it could be a sign of substantial temporal mixing. However, as is shown here, such mixing does not necessarily affect the interpreted model of the data, nor the modelled parameter values, *given that the samples are identically distributed* (see #script in supplementary material for further details).

(ref:05-multi-ln) Twenty series of sequentially aggregated log-normal distributions, starting with 100 data points and 100 more added for each iteration. Parameter values for every group of 100 data points are fixed at $\mu = 4.5$ and $\sigma = 0.4$ (a) or uniformly fluctuating between $4 < \mu < 5$ and $0.3 < \sigma < 0.5$ (b). Both settings give distributions that resemble those of Neolithic house-size distributions. Red lines indicate power-law tails. The series overlap to a large extent, so y axis is plotted with rank rather than normalised cCDF to facilitate readability. Scales are logarithmic

```{r 05-multi-ln, fig.cap="(ref:05-multi-ln)"}
load("Results/fig05_multi_ln.RData")
fig05_multi_ln
```

This situation quickly changes when the samples are made up of differently distributed sub-samples. In Figures \@ref(fig:05-multi-ln)b and \@ref(fig:05-multi-ln-box)b, the same data series are constructed, but allowing for the $\mu$ and $\sigma$ values to fluctuate randomly within very small ranges, between 4 and 5 for the mean and 0.3 and 0.5 for the standard deviation. The resulting distributions then become slightly more skewed over time, and after the first two iterations almost all distributions are modelled to have power-law tails. This is phenomenon of increased variance resulting from analytical lumping is described and further discussed in Perreault [-@perreault2019, pp. 61-79]. In the specific case of house-size distributions, the implication is that if there are significant changes occurring over the time span of the settlement being analysed -- e.g. that houses become larger or smaller over time, or that there is growing or reduced inequality over time -- this will affect the overall distribution with increased variance, potentially leading to false positive power laws. If individual dating of houses is difficult to achieve, changes in the house-size distribution can to some extent be seen using temporal trends in construction techniques or raw material use as proxies. But if these material factors are stable over time and the change in house-size distribution is induced solely by social factors that are more difficult to observe directly, like post-marital residence patterns or kinship structures, there may be no way of distinguishing trends over time without dating houses individually. The issue of temporal resolution is a major concern in any social archaeology, and a pragmatic attempt at dealing with it is given in the following chapter.

(ref:05-multi-ln-box) The same aggregated distributions as above in box-plots, illustrating how the data ranges increase much more slowly with sample size than expected for log-normal distributions. Sample size is 100 for iteration 1 and increases by 100 to 2000 in iteration 20. Fluctuating parameter values (b) increase variance and the probability of finding power-law tails (red points). X axis is logarithmic

```{r 05-multi-ln-box, fig.cap="(ref:05-multi-ln-box)"}
load("Results/fig05_multi_ln_box.RData")
fig05_multi_ln_box
```

\FloatBarrier

## Summary of methodological procedure and tests

The overall goal of this part of the thesis is to identify power-law structures in the house-size distributions of the sampled Neolithic settlements of Linear Pottery and Trypillia material culture. As it was shown in the previous chapter, unlike other common size distribution models like the normal, log-normal and exponential, the power law is characteristic of hierarchically scaling structures, and it is assumed here that when such structures are observed in house-size distributions they are indicative of some sort of socially relevant hierarchy, as they are very unlikely to emerge from simple random additive or multiplicative processes like those relating to the Central Limit Theorem or Gibrat's law. However, a number of caveats have been presented so far.

Firstly, a power-law distribution does not suffice to say what *type* of hierarchy is in play, only that there *is* a hierarchy. The idea that hierarchical structure in society equals despotism is a prejudice that should be kept out of the analysis. Rather, the exact political organisation of the society needs to be studied archaeologically through multiple angles. However, distinguishing between cases where there is and where there is not hierarchy remains still very useful.

Secondly, even though I have strived to follow best practice in terms of statistical methodology, some issues remain. One of these is that sample sizes in the following chapter are probably near the lower limit of what is acceptable for the distribution fitting and selection algorithm to be effective. Testing on synthetic data sets indicated that the tail models should ideally include hundreds of data points, and that at $N \approx 10$ the model selection is unreliable with a high risk of false positive power laws. Furthermore, especially log-normal distributions are known to often produce power-law tails, and in the limited parameter scan provided here there is no obvious pattern between the values of mean and standard deviation and the probability of identifying a power-law tail. The observations done here on synthetic data sets seemingly show that random fluctuations in the tail are sufficient to produce power laws more or less independently of the parameter values. It remains unknown how relevant this issue is for interpreting archaeological house-size data, since the intensity of random fluctuations in house size is difficult to model precisely. It is possible that the frequent (but not constant) power-law behaviour in the tail is a mathematically inherent property of log-normal distributions. The question of the extent to which the presence or absence of power laws in otherwise log-normal house-size distributions confidently translates into presence or absence of social hierarchy should be addressed in future ethnographic or ethnoarchaeological studies.

Lastly, tests on aggregated log-normal distributions with synthetic data indicate that lumping or mixing of data series does not affect the risk of obtaining false positive power laws given that the mixed sub-distributions have identical parameter values. However, if the mixed distributions differ, even by small random fluctuations in the parameter settings, the aggregated data set quickly runs a much higher risk of giving false positive power laws. In the present context, this issue is especially relevant for cases when archaeological settlements are documented primarily through remote sensing, and when a majority of houses lack individual dating, impeding any further separation into coevally existing settlement plans. It should be noted that of all the caveats mentioned here, the main problem is the identification of false positive power laws (so-called "type 1 error"), while failing to recognise actual power laws (false negative or "type 2 error") is seemingly much less of an issue.

The applied distribution fitting algorithm can be summed up by the following (see also #script in online supplementary material for further details):

-   A lower threshold ($x_{min}$) is selected by fitting power-law models to the data by maximum likelihood estimation. The fitting is done recursively from different threshold values, and the one that gives the model with the lowest K-S statistic is selected.

-   Other candidate models for the tail of the distribution are fitted (log-normal, exponential and stretched exponential/Weibull), also by MLE and with the same lower threshold as the power-law model.

-   The best tail model is selected by lowest AICc score. The result shows whether a power-law model gives a better fit to the tail than the other models.

-   Models are calculated for the whole distribution, without lower bound, excluding the power law and stretched exponential, but including the normal distribution. These are also fitted by MLE and selected based on AICc score.

-   Reported values are the selected models for the whole distribution and the tail, as well as model parameter values, sample size (N), size of the tail (N_tail) and proportion of the data included in the tail (Tail_P), as well as the Gini index calculated on the whole distribution

In order to test if observed power laws are only resulting from data aggregation, the same analysis is also performed on separate quarters or neighbourhoods for two settlements where this information is available (Nebelivka and Vráble), and on modelled coeval settlement plans for Vráble. In all cases, samples of size 10 or lower are excluded from the analysis, and results for distribution tails that include 10 or less houses are disregarded.
