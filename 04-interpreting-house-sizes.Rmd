# (PART) Size distributions {.unnumbered}

# House sizes and social meaning {#house-sizes-theory}

## Interpreting house-size differences {#house-sizes-ethno}

In archaeology there are two recurrent and seemingly contradicting assumptions underlying interpretations of house-size differences within a society. In studies where the goal is to provide population size estimates, this is often calculated from living area, where the square metres per inhabitant is modelled from ethnographic analogies. The population of a village is then found by summing together those of every house [e.g. @coudart1998, pp. 79-80]. A simpler and probably more simplistic version of this is to consider a m^2^/inhabitant proportion that is constant no matter the size, and thus calculate the population directly from the total living area of the village (**give examples**). This mean value is generally obtained from multiple ethnographic parallels. With this assumption -- that every inhabitant requires a similar amount of living space -- the size of a house effectively reflects its number of inhabitants. The second assumption, which is more frequently seen in studies focussing on wealth inequality and social stratification, is that house-size differences are expressions of differences in some sort of wealth or power (**give examples**). In this view, a larger house would have belonged to a wealthier household, capable of procuring more raw materials and activating a larger labour force for its construction and maintenance, in which case there would be significant disparities in living space per individual. Even though there is not necessarily any contradiction between these two interpretations from an anthropological point of view, most archaeologists seem to be unable to consider both possibilities simultaneously [@wilk1983]. From a methodological point of view, each of these assumptions will tend to mask our ability to see traces in the archaeological data relating to the other assumption -- that is, with existing methods we cannot convincingly provide estimations of both population size and level of social inequality from the same data, even though house sizes frequently form the basis of both argumentations. Far from proposing a solution to this issue, my argument here is that a variety of social institutions known from ethnography and historical sources can explain some level of correlation between the two variables. Dowry and bride price are geographically and temporally widespread practices that link number of offspring (daughters, sons or both, depending on cultural context) with wealth. Clan leaders may draw upon kinship ties and dependencies in order to obtain the workforce needed to construct a larger house (**citation needed**). In agricultural societies, land ownership is often seen to correlate with household size, since land owners tend to attach workers and servants to them, people who themselves in turn tend to come from landless households [@wilk1982, p. 629; @netting1982]. Nevertheless, if the notion of wealth is at all to be applied meaningfully to non-capitalistic societies, it should designate cases where there is significant and persistent material disparities between members of a population, and not simply point to different household sizes where wealth is proportional. House sizes could thus potentially reflect a somewhat more complex culture-specific interplay between household size and wealth, meaning that for a given house size, one could assume a range of possible values of the two parameters (Figure \@ref(fig:04-intro)). This relationship between household size, wealth and house size should be studied more in detail empirically through the available ethnographic data, rather than reducing its complexity to a mean surface area per inhabitant for the entire population. Such a study, however, lies beyond the scope of the present thesis. Here I will largely leave aside the question of population and household size, focussing on distribution types of house-size data, arguing that the most unequal distribution type considered here -- the power-law distribution -- is unlikely to emerge only from random differences in household size and standard marital patterns, favouring thus interpretations relating to systemic wealth and/or power differences.

(ref:04-intro) House size can be interpreted in terms of number of inhabitants (household size) or material wealth of the inhabitants, but the exact relationship between these two variables remains poorly understood and is probably both complex and culturally contingent. Any given house size within a distribution can thus result from a combination of effects from the two. In many cultural contexts household size and wealth may furthermore be directly correlated

```{r 04-intro, fig.cap="(ref:04-intro)", warning=FALSE}
load("Results/fig04_intro.RData") 
fig04_intro
```

Even though the goal here is not to investigate household sizes but to focus on the material aspect of house-size distributions, some fundamental issues of terminology should be addressed. The use of the word *house* (in the wider material sense rather than the Lévi-Straussian sense, see Section \@ref(social-hierarchy)) is indicative of an underlying assumption that the building in question was in use primarily for domestic purposes -- essentially, a fixed architectural unit where someone would spend their nights at least most of their time, and in many cases also cook and eat their main meal during the day -- though proving this directly is not always straightforward in archaeology. For the cultural contexts discussed here -- the Linear Pottery and Trypillia groups in the Neolithic-Chalcolithic -- there is however little evidence for buildings with specific non-domestic (e.g. economic, religious or administrative) purposes, with the probable exception of the so-called mega-structures or assembly houses in the Trypillia mega-sites, which are discussed more in detail below. This lack of evidence does not imply that there was no specialised economic, religious or administrative activity in these societies, as ethnography and history clearly shows that such activity must reach a certain degree of specialisation before it materialises in distinct buildings devoted exclusively for these functions. Artisans, shamans and chiefs could be specialised to some extent but still perform their activity at their domestic home or more diffusely outdoors or without any fixed location [e.g. @costin1991, p. 25; @kahn2021]. It is in any case of common usage to speak of houses when discussing architectural units in Neolithic Europe and other prehistoric contexts, maybe because of a lack of a better generic word, but this usage should not prevent archaeologists from recognising other non-domestic functions of buildings whenever there is evidence for it. The *household* is furthermore the designation of all the people, genealogically related or not, usually (though not necessarily) living under the same roof or within the same architectural unit, constituting a functional whole economically and socially, and potentially including more than a single family unit, as well as servants or slaves, depending on the context [@wilk1982, p. 620]. The emphasis for defining a household is thus more on its economic and social function than on co-residence and kinship relations, which are somewhat more variable. In the following, I go into some more detail as to how household organisation as well as wealth are known to influence house size.

### House size and household organisation

The focus on houses and households in archaeology -- as opposed to larger units of analysis like whole settlements, cultures and periods -- started to attract momentum by the end of the 1970s, with the work of @wilk1982 often cited as the original manifesto of its validity and importance, pinning "Household Archaeology" as an independent genre of study. In their view, the household could be understood as the most abundant activity group in any society, despite considerable variations as to its importance relative to other types and scales of social groupings. They defined it by their *social*, *material* and *behavioural* constituent elements, that is, its members and their relations, the dwelling and other possessions, and the activities it performs. The material element is of course the only one directly accessible to archaeologists, and the task for researchers would be to reconstruct the social and behavioural elements from the material. This realisation led to ambitious comparative projects of mapping the variability of houses and their occupants in different parts of the world and socio-economical contexts, with inferences onto prehistoric contexts built upon existing ethnographic literature [e.g. @murdock1949] as well as new ethno-archaeological observations [e.g. @wilk1983; @blanton1994].

One of the main characteristics of households is their *size*, which, according to @wilk1982, to some extent is determined by the scale of the production activities that fall within their organisational sphere. In societies where large scale complex tasks necessitating the simultaneous cooperation of many hands are organised at the household level, the optimal household size will be accordingly large. Such activities can include agricultural tasks like irrigation or terracing, as well as house construction. However, when large-scale activities are necessary only once or twice a year, as with seasonal large game hunting or intensive fisheries, they tend to be organised at a community level by many households working together temporally. In such contexts, households can be smaller as their daily activities can be performed by a smaller number of people [@wilk1982, p. 623; see also @hamilton2007 CHECK#]. Furthermore, ethnography has repeatedly shown that large households performing complex activities often need a head coordinator for the activities to run smoothly. However, the set of activities organised at the household level in a society and thus determining the optimal household size, will affect all households similarly unless there is some economic differentiation between them. In societies where households are economically more or less self-sufficient, their size differences should be expected to be random (i.e. normally distributed, see Section \@ref(normal-dist)).

Kinship studies within anthropology have shown over the last decades that in most societies, contrarily to common misconception, kin affiliation is *not* simply a matter of biological relatedness. In an attempt of grouping together all possible justifications for kinship ties, Marshall Sahlins [@sahlins2013] defined kinship as "mutuality of being" -- that some real or imagined substance is shared, and that this substance is not necessarily genes, as is mostly the case in modern Western societies (with adoption as the main exception). A variety of non-genetic foundations of kinship are widely accepted in different cultures, like sharing of name, time or place of birth or childhood, food source, shared experiences, blood ties, and so on. At the same time, archaeology as a whole has arguably been very slow in taking this diversity of kinship configurations into account, far too often taking for granted the modern Western (especially post-war 20th century) ideal of patrilineal nuclear families as the default configuration for all of human history [@ensor2021]. That being said, anthropological kinship studies such as that represented by Sahlins typically shows little concern with material culture, and are mostly silent on the question of who -- at the end of the day, quite literally -- sleeps under the same roof, making it hard to interpret domestic architecture in terms of kinship structure and social organisation. The recent comeback of kinship studies in archaeology has been far more focussed on linking isotope and aDNA data with social structures, largely fuelled by the rapid developments of the related methodologies.

-   Kinship and households: who lives in a house? @ensor2013, @ensor2017.. archaeology: , @madella2013, @joyce2000, @carsten1995, @hofmann2013, @kramer1982

### House size and wealth

Wilk and Rathje pointed to the important role of households in inter-generational transmission of wealth in many societies [-@wilk1982, pp. 627-31]. Specifically, following @murdock1949 and other ethnographers, they argued that as populations grow, land tenure tends to institutionalise at the household -- and later individual -- level around the moment when agricultural land becomes more scarce than labour. Before this -- in pioneer phases of agricultural development -- land is readily available and if there is any concept of land ownership at all, it usually lays at the community level. Once the agricultural land in a region is saturated, rights to use it will tend to be transferred within households, and children from households with extensive rights have a greater incentive to stay within or close to it, while children from households with less land rights are more likely to emigrate. Further population pressure will tend to limit partition of inheritance between siblings, so that land ownership over time is transmitted within a smaller and smaller fraction of the population. This situation is also shown to entail stratified (and parentally arranged) marriage, further entrenching social stratification.

Maya here!

@kahn2021 describes an example from eastern Polynesia of agricultural expansion and intensification developing over 400 years from the initial colonisation before social stratification starts to become materialised in differential architecture. In later phases, she links the appearance of specialised buildings devoted to communal assemblies, rituals as well as residential and ritual buildings for elites with the emergence of complex chiefdoms. Though quantitative measures of house size are not presented in the study, it clearly demonstrates that elite architecture was consistently larger than that of commoners in the prehistoric Society Islands. However, she also identifies types of houses of special economic function like workshops, and points out that these can be hard to distinguish from common residential houses archaeologically simply by looking at size [@kahn2021, pp. 90-1]. In such cases -- where there is a range of non-residential building types -- it may be of little use to analyse house-size distributions as single blocks. It may then be necessary to first group buildings into functional types based on internal structure and finds inventories, given that this information is available. This issue concerning the prerequisites of data input for the analyses done in this thesis, is further discussed in Chapter \@ref(disc-methods). It should also be noted that interpretations regarding the social structure of pre-contact Polynesian societies are greatly aided by near-contemporary written accounts from the first European explorers in the region, and house floor levels are often well-preserved due to the relatively recent dates of the structures (10^th^ to 18^th^ centuries CE) -- both factors being in stark contrast to the central European Neolithic, where the spatial organisation and sizes of houses are often nearly the only information available.

Regarding the domestic architecture of clan leaders and chiefs, several studies have highlighted how elites both have the material means and the socio-economic incentives to build houses that are more monumental than those of commoners HAYDEN1997++. Others have warned that this should not lead archaeologists to systematically interpret large buildings as evidence of elite-based top-down social hierarchies, since there are also ethnographic examples of monumental buildings being constructed collectively by more egalitarian communities for assemblies or other communal activities. Referencing a range of ethnographic and archaeological examples, @goodale2021 point out that the act of constructing a monument may be organised and to some extent coerced by a leader, or equally by a collective group. In both cases the intended function of the finished building will serve in the interest of the one or those who initiated the project. This is furthermore valid at all social scales -- the constructuion of a residential building is initiated and driven by those who wish to live in it, and the construction of a ritual building for the village like an assembly house is initiated and driven by the entire village. Furthermore, the construction of a palace in a state society is initiated by the leader and financed through the tribute collected throughout their effective territory. Consequently, buildings should be expected to somewhat reflect the scale of their social importance through the level of effort put into their construction, so that in a hierarchically organised society (be it top-down coercive or bottom-up collective), this effort should also be hierarchically distributed in its buildings. I argue here that building size is the best (and often the only) available proxy to this construction effort (see Section \@ref(power-law) below for more discussion on this point).

Do clan leaders have bigger houses? check @haude2019, @bradley2013, @wilk1983. @smith1987.

-   General tendencies for Pueblo contexts: @dohm1990

P. Květina and J. Řídký point out both architecture (construction, size, orientation) and settlement layout as possible distinctive features between Big Men (achievement-based) and Chief societies, arguing that the former type may be recognised by a dispersed intra-settlement layout combined with uniform architecture, while the latter type would tend towards more regular settlement layout and more marked differences in architecture [@kvetina2019, p. 13]. EXPAND UPON THIS.

-   Schiesberg 2010 2016, go through refs in Zotero, family size and houses for the LBK.

Functional difference:

-   Ethnography of initiation houses, communal/assembly houses, ritual houses, including @barley2011, @godelier1986, @wilk1983, @fraser1968, @haude2019

Some factors other than household size and wealth have been suggested and empirically reported to systematically influence house size [@wilk1983, p. 101]. Among these are:

-   Mobility -- Seasonally mobile groups tend to build smaller dwellings than more sedentary groups [@porcic2012]

-   Post-marital residence -- Houses in matrilocal societies tend to be larger on average than in patrilocal societies [@hrncir2020; @porcic2010]

-   Climate -- Houses are smaller in cold circumpolar or mountainous regions because of the cost of heating

-   Duration of residence -- Households that have been established at a location for a long time or are well integrated in the community tend to have larger houses [@wilk1983]

-   Material use and technology -- All building materials have associated constraints and costs, and innovations can lead to larger constructions at lower costs

Of these factors, only the duration of residence will have a direct influence on house-size difference *within* communities. Mobility, residence patterns and climate are more constant factors affecting entire communities and will thus affect the average house size, but not the level of inequality. Access to building materials and technology on the other hand may be differentiated in stratified societies, and lead to unequal house sizes as a materialisation of wealth. A related factor that may lead to a certain under-representation of the wealthiest households, is the construction of multi-floored houses, unless this feature is recognised archaeologically and included in the calculation of house sizes. For the case studies presented in this thesis however, building materials and techniques are not noticeably differentiated between small and large houses, and the possibility of multi-floored houses is only a minor issue that has been discussed in Chapter \@ref(material).

Lastly, in his ethnographic comparison between egalitarian and more ranked contemporary Maya village communities in Belize, Wilk [-@wilk1983, pp. 111-4] also pointed to social norms potentially *preventing* differences in public display of wealth even when such differences existed. He linked such norms to the openness of the village economies -- in closed self-sufficient villages there was greater mutual dependence between households, and wealth display was strongly discouraged, potentially sanctioned with witchcraft, whereas in villages with more open economies household wealth was more readily displayed through house size and the quality of construction materials.

## Distribution types and their underlying mechanisms {#distributions}

The main question in this part of the thesis is on the nature of house-size differences in the studied contexts, and to explain them either as being due to random fluctuations or as a material expression of a more structural inequality, or something in-between. In statistical terms, this is a question of distributions. A distribution is a mathematical model of the spread of data along a variable or axis, and it can be modelled on empirical data as a succinct description, or used to predict unobserved data (e.g. future developments, or, in the case of archaeology, data that is lost to taphonomy). In theory, there is an infinite number of possible distributions, as there is no limit to how many parameters one can include to fit the data. It is however generally considered good practice in statistics to limit the number of parameters and identify the simplest possible model that gives a good fit, since a larger number of parameters can often lead to a better fit, but at the same time be harder to explain in terms of underlying mechanisms. Adding many parameters only to achieve a marginally better fit is referred to as *overfitting* (#ref?), and for most real-world contexts there is a limited number of model types that can be considered reasonable candidates. In cases where we are interested in inequality or uneven distributions of data (so-called *skewed* distributions), the most likely distribution models are those that can be described as *heavy-tailed*, meaning that on typical graphical representations (like density/PDF plots or histograms) they will show a characteristic stretch of some of the data towards the right end of the x-axis, while most of the data remains on the left side (Figure \@ref(fig:04-PDF)). The opposite orientation is also possible in theory, in which case the distribution can be referred to as left-tailed. It is important to keep in mind however, that not all distributions are heavy-tailed, and that other distributions also model the spread of data across a variable, but result from very different underlying mechanisms. Identifying the most likely distribution model for a data series is therefore crucial for understanding how the data could have been generated. And though it is true that one model type can have multiple different explanations -- in an archaeological setting for example, many different social behaviours can lead to the same material outcome, an issue known as *equifinality* -- excluding one or more model types for the observed data can help limiting the number of plausible interpretations considerably.

Distributions can be represented graphically in a number of ways, most commonly in a coordinate system with the variable on the x-axis and its density, or *probability density function* (PDF), on the y-axis (Figure \@ref(fig:04-PDF)). Often, and in many of the sources cited in this part of the thesis [e.g. @clauset2009], the PDF is denoted by $p(x)$, reading as *the probability of x*, or simply $f(x)$ (*the function of x*). It gives the probability for a drawn sample ($X$) of falling within a given arbitrarily short range of the distribution, written $Pr(x \le X < x+dx)$. By definition, the area between the PDF curve and the x-axis sums to 1. For reasons that are further discussed in Chapter \@ref(methods-distfit), heavy-tailed distributions, and power laws in particular, are instead often represented with their *cumulative distribution function* (CDF), which is the integral of the PDF (inversely the PDF is the derivative of the CDF; Figure \@ref(fig:04-cCDF)). Similarly to the PDF, the CDF is often denoted as $P(x)$, depending on disciplinary tradition. It indicates the probability of a random sample value being equal to or lower than the function value, or $Pr(X \le x)$. Furthermore, the specific version of the CDF used for plotting heavy-tailed distributions, is the *complementary* or upper tail CDF (sometimes referred to as the *survival function*, or denoted cCDF), which is $1-CDF$ or $P(X \ge x)$, indicating the probability that a random sample is higher than the function value. Both axes on such cCDF plots are traditionally set on logarithmic scales, usually $\log_{10}$ for readability. To avoid confusion, in this thesis I refer to PDF and cCDF for density and distribution functions respectively (except in equations, where I use $p(x)$ and $P(x)$ respectively), and all PDFs are plotted on linear scales and cCDFs on $\log_{10}$ scales, unless otherwise stated. Apart from scales on plots, whenever I refer to logarithms (i.e. in calculations), I imply natural logarithms, that is $\ln$ or $\log_e$ where the base number $e \approx 2.718$.

(ref:04-PDF) Example curves of the probability density function (PDF) of four common distribution types: normal (blue, $\mu = 50$, $\sigma = 8$), exponential (red, $\lambda = 0.1$), log-normal (green, $\mu = 3$, $\sigma = 0.5$) and power-law (purple, $\alpha = 3$, $x_{min} = 1$), in linear (a) and logarithmic scales (b). Parameter values are arbitrary and x-axis is truncated at $2 < x < 100$ for readability. The power-law distribution is the only to form a straight line when both scales are logarithmic

```{r 04-PDF, fig.cap="(ref:04-PDF)"}
load("Results/fig04_PDF.RData")
fig04_PDF
```

(ref:04-cCDF) The same distributions as in Figure \@ref(fig:04-PDF), but with the complementary (right-tail) cumulative distribution function (cCDF), in linear (a) and logarithmic scales (b), with $0 < x < 80$ for readability

```{r 04-cCDF, fig.cap="(ref:04-cCDF)"}
load("Results/fig04_cCDF.RData")
fig04_cCDF
```

In the following, I present briefly the main distribution types that will be discussed further in the following chapters, with special focus on the *power-law distribution*, which is the model type associated with fractals and structural hierarchy. All of these distributions are modelled on continuous univariate data series [see @johnson1994 for more detailed presentations]. Even though their mathematical definitions may seem complicated to non-initiated readers, most of the distribution types discussed in this thesis are readily implemented in standard statistical software, including Microsoft Excel, allowing for a more straight-forward use of them. For this thesis, I used base *R* functions for calculating PDFs and cCDFs, and for random number generation for normal, log-normal and exponential distributions [@R2023], and equivalent functions from the *poweRlaw* package for power-law and Weibull/stretched exponential distributions [@gillespie2015].

\FloatBarrier

### Normal distributions and the Central Limit Theorem {#normal-dist}

One of the distribution models that are the most commonly referred to and well-known, is the *normal distribution*, also known as the "Bell Curve" due to the characteristic bell shape of its PDF (Figure \@ref(fig:04-PDF)a), or Gaussian after mathematician C.F. Gauss who contributed to its exploration in the early 19^th^ century. Its characterising parameters are the *mean* (denoted $\mu$, *mu*) and *standard deviation* ($\sigma$, *sigma*), and the PDF is defined mathematically as

```{=tex}
\begin{equation}
p(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-(x-\mu)^2/2\sigma^2}
(\#eq:normal)
\end{equation}
```
where $e$ (*Euler's number*) and $\pi$ (*pi*) are mathematical constants [@johnson1994, pp. 80-8]. This distribution is centred around its mean, with dwindling amounts of data spread outwards to either side. The mean -- commonly known and widely used in daily speech -- is the sum of all observations divided by number of observations, or

```{=tex}
\begin{equation}
\mu = (\sum_{i=1}^nx_i)\,\frac{1}{n}.
(\#eq:mu)
\end{equation}
```
The spread of the data from the mean is defined by the standard deviation, which is the square root of the mean of all squared deviations from the overall mean, or

```{=tex}
\begin{equation}
\sigma = \sqrt{(\sum_{i=1}^n(x_i-\mu)^2)\,\frac{1}{n}}.
(\#eq:sigma)
\end{equation}
```
Mathematically, the square of the standard deviation, $\sigma^2$, or *variance*, is simpler, but since it is also less intuitive, I refer here to the former whenever possible. The standard deviation can be thought of as the mean of all deviations, positive or negative, from the overall mean.

The great importance the normal distribution has to a wide range of phenomena is explained through the *Central Limit Theorem* (CLT), according to which the sum of random variables tends to a normal distribution as the number of variables increases towards infinity, under certain conditions [@johnson1994, pp. 85-8]. More specifically, if $X_1, X_2, \dots, X_n$ are independently drawn and identically distributed (condition referred to as *i.i.d*) random variables or samples, their sum will be normally distributed in the limit as $n$ tends to infinity, or

```{=tex}
\begin{equation}
\lim\limits_{n \to \infty} \sum_{i=1}^n X_i = \mathcal{N}.
(\#eq:CLT)
\end{equation}
```
The sum may be standardized in some way to avoid infinite numbers, but in practice $n$ will always be limited, so the resulting distribution will always also be approximately normal at best. The original distribution of the random variables (i.e. that of the *population*) does not need to be normal for the theorem to hold, but can be any distribution as long as its variance is finite. The normal distribution (referred to as the *sample distribution* in this context) is related to the population distribution of the random variables (samples), in that the mean of the population equals the mean sum of the sample distribution divided by $n$, and the standard deviation of the sample distribution, referred to as the standard error $s$, equals the standard deviation of the population divided by the square root of $n$, or $s = \frac{\sigma}{\sqrt{n}}$, meaning that $s$ is reduced with higher $n$ (i.e. larger sample size). A common variant of the CLT is that the distribution of sample means, rather than sums, are normally distributed, in which case the mean of the sample distribution equals the mean of the population distribution.

In more laypersons' terms, the total weight of a box of strawberries is the sum of the weights of the strawberries it contains (subtract the weight of the box itself, and assume the same number of berries per box for the sake of argument). In the packaging facility, a large population of strawberries are continuously distributed randomly into boxes of the same size. The berries in the factory (the population) have weights that follow some distribution with limited variance -- some are bigger than others, but there are upper and lower limits to how much a strawberry can weigh -- and all the berries in the boxes are drawn from this same population (so identically distributed). New berries are shipped to the facility all the time, and berries are not sorted according to size but randomly mixed, so if one box by chance gets filled with only very large berries, that does not affect the weight of subsequent boxes (each box is the sum of $n$ independently drawn sample berries $X$). Under these circumstances, the weight of boxes of $n$ berries (i.e. the sum of the berries' weights, the sample distribution) will be normally distributed by the Central Limit Theorem.

Some details here are crucial: the distribution of the strawberry weights themselves will, with higher $n$, tend towards that of the population, which is not necessarily normal. The theorem is only valid for summary measures like sums or means, and not for the observations directly. If the strawberries at the facility are mostly large (heavy) and with only smaller proportions of small berries, this skewed distribution will be reflected in strawberry boxes of a certain size ($n$), but since the influence of this distribution is the same on all boxes of the same size, their overall weights (or mean weights) will be normally distributed. Furthermore, if we draw some boxes from one producer and some from another producer who has significantly larger or smaller berries, the samples are then drawn from different populations and are thus not identically distributed, and the sample distribution will not necessarily be normal (colloquially referred to as "comparing apples and oranges"). Similarly, if there is an overall trend of boxes becoming heavier over the season, then samples from across the season will not be normally distributed. If however, we compare the mean box weights from multiple seasons, these will again be normally distributed, unless there is also a multi-year trend of strawberries becoming larger or smaller. In other words, the i.i.d. condition of the CLT means that the samples are drawn at random, with no important underlying trends. As a side note, there is a substantial body of research on the conditions under which normal distributions can emerge *without* the i.i.d. condition being met [see @johnson1994, pp. 87-8 for details and further references].

Also important to note is that the number of samples $X_n$ (boxes of sample size $n$ in the example above) does not matter to the shape of the sample distribution, other than to the resolution of the curve or binwidth of the histogram when plotting, and the weight of a single sample can be modelled as a probability following a distribution. The normal curve of the sample distribution can be entirely defined by the population $\mu$ and $\sigma$, and sample size $n$. In many practical settings however, the parameter values of the population distribution (and even its type of distribution) are unknown, in which case more samples are needed in order to model a sample distribution and from there infer the parameter values of the overall population. This is the case when statistical tests like Student's t-test or ANOVA are applied for examining the relations between samples and populations.

In other cases, $n$ (sample size) can be unknown, but assumed to be approximately the same for all $X_n$ (samples), like in the somewhat more abstract cases where each of the $n$ variables contained in $X_n$ are of different nature -- or stated otherwise, when the size of $X_n$ is the sum of many different and independent causes. In the case of a normal house-size distribution within a given cultural setting (like a village), one can assume that the same number of causes affect the size each house takes when constructed, but to varying degrees (colloquially we often say *factors* for such causes, though in this setting one should strictly speaking prefer *terms*, *summands* or *addends*, since they add up rather than multiply). Say that house size in a given context results from the cumulative effects of household size, inherited wealth, soil quality, exposure to sun, wind or flooding, artisan specialisation, raw material availability in the year of construction and many more variables. Each of these may have separate probability distributions -- e.g. the wealth distribution may be heavy-tailed while household size may be normal and symmetric -- but as long as the overall population, so to speak, of contributing causes is distributed in the same way to all households, and that none of the variables dominates the effect of the others, and the value of each variable is independent from the values of the other variables, their sums expressed in house sizes should be normally distributed by the CLT. Such a distribution is then an expression of *random difference*, and in the case of house sizes, there would be no particular reason as to why a few houses would be bigger than all the others, just as a few houses would also be smaller, while most would be centred around the mean size. But again, if samples are drawn from *different* populations, e.g. houses from different villages or cultural contexts -- where the probability distributions of the underlying variables are categorically different -- the i.i.d. condition is not met, and the distribution of house sizes is unlikely to be normal. Likewise if the population contains grades, i.e. is grouped, or where the different variables are correlated between them, e.g. if wealthier households are also larger households and have more access to raw material and better quality soils, and so on, their house sizes may also become disproportionately large and deviate from normal expectations.

A final caveat for normal house-size distributions that may be mistaken for skewed ones, is the case when there are no significant differences between house sizes, except for one that has a clearly different function -- as in the ethnographic cases of community houses and men's houses discussed above. Then it makes little sense to interpret the resulting slight skewness of the distribution as a sign of social inequality in itself (notwithstanding gender inequalities). As a rule of thumb, to avoid such misinterpretations, it is useful to test whether isolating the single largest house changes the retained model when performing distribution fitting.

### Exponential distributions and constant rates of growth and decay

The exponential distribution is -- next to the normal -- one of the most widely applicable distribution models. In its simplest form, it is a function of a positive random variable $x$ where some base number (usually $e \approx 2.718$, for compound continuous growth) is raised to the power of $-x$, in other words when $x$ has the probability density $$p(x) = e^{-x}.$$

This simple form is called the *standard* exponential distribution, and in most practical applications there will also be a *rate* parameter $\lambda$ (*lambda*), so that the density function becomes

```{=tex}
\begin{equation} p(x) = \lambda e^{-\lambda x}.
(\#eq:exponential)
\end{equation}
```
In the case of the standard version, $\lambda = 1$ and can be left out. The negative rate in the exponent is the actual rate that determines the shape of the distribution, whereas the rate multiplier to the base is a *normalising constant* which assures that the area under the curve adds up to 1, and thus that the values shown on the y-axis are probabilities. This constant can be though of as the y-intercept of a linear model, since at $x = 0$ the function gives $\lambda e^0 = \lambda\,1$. This is seen more clearly if we take the logarithm of the exponential density function, $p(log(x)) = log(e)\,(-\lambda x) + log(\lambda) = -\lambda x+log(\lambda)$, which is a linear model with slope $-\lambda$ and $log(\lambda)$ as y-intercept. As a rule of thumb, an exponential distribution can thus also be recognised as a straight line on a plot with one linear and one logarithmic axis. A wide variety of more complex forms have been formulated, and the distribution type is furthermore generalisable to both the Gamma and Weibull distributions [@johnson1994, pp. 494-9]. Note that the simple form presented here may also have more complex notations in specialised statistical literature, *cf*. Eq. 19.1 in @johnson1994, which is equivalent to the density function above given that $\sigma^{-1} = \lambda$ and $\theta = 0$ [see also @johnson1994, pp. 522-3; @clauset2009, p. 664].

Exponential distributions have the highest probability (so the most data) at low values of $x$, and ever lower probabilities towards the right end of the curve (Figures \@ref(fig:04-PDF)a and @ref(fig:04-cCDF)a), with the rate of decrease determined by $\lambda$ -- the higher the rate value the steeper the curve falls off from left to right, and inversely low $\lambda$ values give more heavy-tailed distributions. The type of setting which is most commonly modelled as an exponential distribution, is that of "events recurring at random in time" [@johnson1994, p.494]. If $x$ represents the the duration in time between events (or duration of single events) that occur continuously and independently from each other, with a constant average rate of $\lambda$ events per unit of time, it can be modelled as an exponential distribution. An important feature of the underlying process (a so-called Poisson point process), is that it is memoryless, meaning that the duration between events $X_1$ and $X_2$ does in no way affect the duration between $X_2$ and $X_3$, as all durations are drawn independently from the distribution with the same average rate $\lambda$. The example of such a process that may be the most familiar to archaeologists, is the radioactive decay of the ^14^C isotope with its average decay rate $\lambda \approx 0.00012$ or 0.012% per year. With a rate that low and using years as the time unit, it is more useful and intuitive to work with the *half-life* measure, or the time it will take on average for the initial quantity to be halved, which for ^14^C is about 5730 years. The half-life (the median of the distribution) is given by $\log (2)/\lambda$, solving for $x$ in the cCDF function $e^{-\lambda x} = 1/2$, where the normalising constant in the PDF is replaced with 1 (and therefore omitted in multiplication) for the initial quantity or y-intercept. If we replace $1/2$ with the remaining proportion of ^14^C in the organic material of an ancient artefact, compared to the expected amount in the same material when alive, we can use the same equation to solve for the approximate year when the organic material died, which is the principle behind radiocarbon dating. In a radioactive decay process, a total amount of individual unstable isotopes are present from the start (here the death of an organic material), and their individual lifetimes until decay are exponentially distributed. As an additional metric of exponential random variables, the mean $\mu$ or *expected value* is given as the reciprocal of $\lambda$, so that $\mu = 1/\lambda$ and $\lambda = 1/\mu$. The mean is larger than the half-life, and for ^14^C decay, this corresponds to $\mu \approx 1/0.00012 \approx 8267$ years.

The exponential distribution can also model many processes that are closer to an everyday human scale than ^14^C decay. Expanding from the example used for normal distributions, let $\lambda$ be the average risk for a strawberry of being harvested within a week $x$. As the weeks go by (as $x$ increases), the cumulative risk of being picked grows exponentially, so that there are very few berries that are older than a few weeks, and the mean age of the berries in the field is $1/\lambda$. The berries are furthermore picked by a harvesting machine that is unable to aim for a certain size category, and the berries grow linearly (which may be a rather poor approximation, but for the sake of argument). The harvested berries are also continuously replaced by new berries which start growing at the same pace, so the field is always renewed. Under these circumstances, the lifetime of a strawberry during which it grows is exponentially distributed, and the probability of surviving $x$ weeks follows Equation \@ref(eq:exponential). The example illustrates how exponential functions model repeated multiplication or multiplicative processes, since the $x$ in the exponent means "multiplied $x$ number of times". The rate $\lambda$ (technically $e^{-\lambda}$ in the case of continuous decay) is multiplied with itself $x$ number of times (keep in mind that $(a^b)^c = a^{bc}$). Multiplying the rate for each new step in time means that the value of $\lambda$ is applied to the current value of $x$ rather than to the initial value. For example, let $\lambda$ be $1/5$ or $0.2$, so that there is a 20% risk of being harvested within a week, and thus 80% chance of being left in the field. The expected lifetime of a strawberry is then $5$ weeks, and the probability of surviving $6$ weeks or more is $P_X(6) = e^{-(1/5)6} \approx 0.301$ or 30,1%, while that of surviving $7$ weeks or more is $P_X(7) = e^{-(1/5)7} \approx 0.247$ or 24,7%, corresponding to a relative decrease in probability of $\frac{(0.301-0.247)\,100}{0.301} \approx 18$ %, equal to $1-e^{-1/5}$. Note that using the rate directly as the base, rather than as exponent of $e$, will give the same results as a (discrete) geometric distribution, in which case the reduction would be exactly the rate between each period. The difference lies in what is termed compound and simple interest in economics. For all the purposes discussed in this thesis, the continuous exponential distribution (with $e^{-\lambda}$ as base to $x$) is deemed more appropriate than the discrete geometric distribution.

From an archaeological point of view, the essential point from the strawberry example above is that this process will also materialise in the size distributions of each single harvest, of all the strawberries at the depot, as well as the strawberries in a finished box for sale, all of which will be exponentially distributed (unless there is some additional sorting process involved). Of course, this does not change the fact that the box weights, as well as the mean strawberry size per box will be normally distributed, as previously shown. When it comes to house sizes, several scenarios involving growth could explain an exponential distribution. Let house size ($x$) be directly dependent on household size, so that each inhabitant has a constant average number of m^2^ of roofed space. Say that the households grow exponentially at some rate, that is they increase in size by a factor of $\lambda_1$ each cycle of some unit length ($y$). At the same rate, the households also extend their houses proportionally to their growth, or replace their house altogether with a bigger house, and lastly, for each new cycle a new household is added to the village with newcomers, so that the total number of households follows $y$ linearly. If all new households start at some minimal size $x_{min}$, the size $x$ of a house after $y$ cycles will equal $x_{min}e^{\lambda_1 y}$, and we can solve for its age (time since establishment of the household) as $y = \frac{\log(x)-\log(x_{min})}{\lambda_1}$. Here I use $x_{min}$ for $\theta$ in @johnson1994, p. 494, following @clauset2009, p. 664. In other words, this context would generate an exponential house-size distribution of coeval houses in a village, where the largest houses are those of the households which were the first to settle in the village.

This model is of course not very realistic. For example, households cannot grow without limit, so the larger they become, the higher the probability that they split into two or more factions (see Alberti 2014 and Johnson 1982, and Section \@ref(social-hierarchy)). The splitting of households itself can in fact also generate an exponential distribution. Let the households in a village grow linearly -- say they each have a constant surplus of 1 person per year (persons who arrive or are born - persons who leave or die = 1) -- but they also run a risk of $\lambda_2$ (e.g. of 5%) of disintegrating and being replaced by a minimal sized household ($x_{min}$) each year ($y$), no matter their current size. House sizes are then linearly correlated with their age (or time since establishment of the household) so that $y = x - x_{min}$ (disregarding the constant of m^2^/inhabitant). However, over time, the probability that a household continues to exist without splitting, will decrease exponentially, and we can write the survival function for households (and therefore their sizes) as $P(X > x) = e^{-\lambda_2 y}$, or $e^{-\lambda_2 (x-x_{min})}$.

But again this model is not very satisfactory, particularly since it assumes purely linear population growth, which is unlikely in most cases. It would also seem likely that the probability of splitting of households would not be the same across the range of sizes, but rather be concentrated around some upper threshold, determined by the social structure between the inhabitants or by material or ecological constraints (or a combination). Common for both of the models above, is that one aspect is well described as exponential, but this aspect is only one out of many in the complex process which may underlie the house-size distribution of a settlement. Intuitively, it would also seem strange to have the whole range of houses in a settlement to scale exponentially, since it would imply that single house sizes would be ever closer and closer to each other all the way down to the smallest house in the village. Adding the $x_{min}$ parameter does change this situation though, as it could then apply to cases within some upper class in which household size and/or wealth would increase to some fixed rate over time, not affecting the size of the main part of the settlement's households, which could well be normally distributed. Or the two models above could be combined to one, pulling exponentially in opposite directions (note that the two rates, $\lambda_1$ and $\lambda_2$ are positive and negative respectively). In both cases, the resulting house-size distribution would no longer be exponential, and these common combination distributions -- the log-normal and the power law, both of which are heavy-tailed -- are presented in more detail below.

Another issue with exponential distributions resulting from growth or decay in time, from the archaeological point of view, is that the time-averaging that so often infiltrates our analyses because of the difficulty of distinguishing temporally coeval data sets, may very well influence the observed data distribution. This influence is much more challenging to evaluate theoretically however, so in this thesis it is instead addressed empirically through simulation in Section \@ref(distfit-synth).

### Log-normal distributions and Gibrat's law

One of the main candidate models for heavy-tailed continuous distributions is the log-normal, defined as a variable $x$ of which the logarithm is normally distributed. Adapted from Equation \@ref(eq:normal), its density function can be written [following the notation in @mitzenmacher2004, p. 229] as

```{=tex}
\begin{equation}
p(x)=\frac{1}{\sqrt{2\pi}\sigma x}e^{-(\log(x)-\mu)^2/2\sigma^2}.
(\#eq:log-normal)
\end{equation}
```
The $\mu$ (mean) and $\sigma$ (standard deviation) parameters are usually understood as the equivalent values associated with the normal distribution of $\log(x)$. It can be thought of as a combination of the normal and the exponential distributions, and like these, it has been shown to apply well to a wide range of natural and social phenomena, from the growth of organisms in biology to the pricing of options in finance [see @mitzenmacher2004, pp. 235-7 and @johnson1994, pp. 209-11, 238-40]. One implication of $\log (x)$ being normally distributed, is that the density curve of $x$ will appear as a normal bell curve when plotted with a logarithmic scale on the x-axis (Figure \@ref(fig:04-lnorm-exp)). With linear scales, the curve is skewed with the mode to the left and a tail of high values to the right. More technically, the (natural) logarithm of a variable ($x$) is the variable of exponents that may raise $e$ to the values of $x$. A linear increment in a variable of exponents -- say from 1 to 2 -- will, with the same base, correspond to an exponential increment in powers (the result of exponentiation), as $e^1 \approx 2.718$ and $e^2 \approx 7.389$. Thus, if the exponents of $e$ that correspond to the values of $x$ are normally distributed, then $x$ itself will resemble an exponentially stretched normal distribution, which is a log-normal distribution.

Since exponents represent repeated multiplication and normal distributions result from random additive processes (see above), log-normal distributions may be most easily understood as resulting from random multiplicative processes. The product rule of logarithms states that the logarithm of a product of numbers equals the sum of the logarithms of those same numbers. This can be expressed as $\log (ab) = \log (a) + \log (b)$. Then, since the sums of many random numbers are normally distributed according to the Central Limit Theorem, the logarithm of the product of many random numbers (that is, these numbers multiplied together) should also be normally distributed [see e.g. @newman2005, pp. 347-8 for more elaboration].

The product of many random numbers is typically the result of a growth or decay process where the rate fluctuates randomly. A convenient example, following @newman2005 [p. 348], is that of a financial investment. If an initial value ($a$) is invested in stocks that generate a return ($e^\lambda$) which fluctuates randomly from year to year with a finite variance, the return $y$ after $x$ years will follow a wiggly exponential curve, or $y = a e^{\lambda x}$ (Figure \@ref(fig:04-multi-exp)). After some years, the value of y will follow a log-normal probability distribution (Figure \@ref(fig:04-lnorm-exp)). If several persons start investing the same amount in stocks at the same time, then after a period, say of 10 years, most of them will have earned returns of comparable size, centred around some mean return, while the earnings of the top investor may be several orders of magnitude higher, simply by chance. This assumes however that everyone invests randomly in the stock market, which is rarely the case. More scrupulous investment strategies may reduce the effects of chance and thus the spread of final returns, but this effect may again be countered by the risk-willingness of investors. In either case the resulting distribution after a given time period will be log-normal, which explains why this model is a central tool in financial analyses [e.g. see @mitzenmacher2004, p. 236 for its use in the Black-Scholes option pricing model].

(ref:04-multi-exp) Exponential distribution of $y = \lambda ^x$ with rate ($\lambda$) fluctuating randomly and uniformly between 0.75 and 1.4, i.e. with a mean rate of approx. 1.08, over 40 periods ($x$) from an initial value of 1. The plots figure 100 individual runs of the distribution, with linear (a) and logarithmic (b) y-axis. Over time, y-values at any given x are expected to be log-normally distributed by the Central Limit Theorem

```{r 04-multi-exp, fig.cap="(ref:04-multi-exp)"}
load("Results/fig04_multi_exp.RData")
fig04_multi_exp
```

(ref:04-lnorm-exp) Density function of y-values from Figure \@ref(fig:04-multi-exp) at $x = 40$, with linear (a) and logarithmic (b) x-axis, following a typical log-normal distribution

```{r 04-lnorm-exp, fig.cap="(ref:04-lnorm-exp)"}
load("Results/fig04_lnorm_exp.RData")
fig04_lnorm_exp
```

This process is known as *Gibrat's law*, after the French engineer Robert Gibrat who was the first to demonstrate its wide applicability, offering a mathematical explanation of the skewed size distributions that had frequently been observed by economists [@gibrat1930]. Gibrat argued that the log-normal distribution was better fit for modelling firm sizes and salaries than the one already proposed by Pareto (i.e. the power law, see below), since it could account for the entire size range and not only the tail, and since it was theoretically better founded, as Pareto's original model was purely empirical. He termed the model the *law of proportionate effect*, since it emerges -- as shown above -- when proportionate growth rates are independent of absolute size. This essentially means that growth is exponential (size at any given time is multiplied with a rate, so proportional) rather than linear (additive), and that the range of possible rates is not determined by absolute size. Note however that there must be some randomness in the rate for the growth to result in a log-normal distribution. If the rate is *exactly* the same for all samples, or if it follows the exact same sequence of random rates, the initial distribution will remain the same over time, even though any initial spread will be scaled up or down according to the rates (Figure \@ref(fig:04-still-normal)). In most economic settings, though there are overall trends that may affect everyone in a population (of firms, employees, cities etc.) at large scales, there are also many smaller factors that will affect individuals differently, causing random variation.

(ref:04-still-normal) a: Eponential growth of 100 samples drawn from a normally distributed initial population ($\mu = 10$, $\sigma = 2$), all following the same sequence of uniformly distributed rates ($0.75 < \lambda < 1.4$). b: Over time, $\mu$ and $\sigma$ values change, but the distribution remains normal. Scales are linear

```{r 04-still-normal, fig.cap="(ref:04-still-normal)"}
load("Results/fig04_still_normal.RData")
fig04_still_normal
```

The modern financial market is of course not directly applicable to the Neolithic, but a number of multiplicative processes involving random fluctuations may also be relevant to Neolithic social structure or economy. One obvious example would be that of crop yields over time. Assuming that crop cultivation in a village is organised at a household level, and that households grow their crops at separate locations in the vicinity, random differences in soil quality, sunlight, water, exposure to disease and so on, would arguably generate normally distributed yields in one year (the yield volume being the sum of many random effects). But given that the yield the year after also depends on the current yield through the size of the surplus that will be available for sowing, a randomly large yield one year will have better chances of producing an even larger yield the next year, and so on, in the same way a large financial return of a lucky investor one year is more likely to reach an even higher return later if reinvested. Supposing again that house size reflects household size linearly, and that larger yields can sustain larger households, this process could explain the emergence of a log-normal house-size distribution in a village over time.

For Neolithic crop cultivation it is in many cases more reasonable to assume that cultivation took place very close to or even within the village, in which case the conditions for yield volume would be very similar between households (#ref needed?). Cultivation could also be organised collectively rather than at the single household level, or as a combination depending on the crop. In such cases, a good or a bad harvest would affect all households equally, and skewed distributions would not emerge easily (#Ref. to Kohler 2018). However, even in such scenarios, small random differences in the initial sowing volumes of individual households could grow exponentially over time and produce a log-normal distribution of household yields, though with smaller spread between the highest and lowest values.

A somewhat different mechanism for explaining skewed house-size distributions within Linear Pottery settlements in particular, was proposed by Sara @schiesberg2010. Assuming a stable population over time, with number of children per woman surviving to reproductive age being Poisson distributed with $\lambda = 2$ (with ever decreasing probability of larger numbers of surviving siblings), and the probabilities of having given ratios of male and female children following a binomial distribution, the combined probability of having *x* male children surviving to reproductive age would follow a skewed log-normal-like (though discrete) distribution. Schiesberg observed this theoretical distribution of male siblings to be analogous to the empirical house-size distribution of excavated Linear Pottery settlements on the Aldenhoven plateau in North Rhine-Westphalia, Germany, arguing that there were gaps in the continuous size distribution that fitted with the discrete limits between numbers of male siblings. The correlation was furthermore explained as a result of a mainly patrilocal residence pattern, where house size would be a function of number of sons in an extended family (patrilocality being the most widely accepted residence pattern for the Linear Pottery culture, see Section \@ref(lbk)). The same pattern could have emerged as a function of number of daughters in the case of matrilocality. Though Schiesberg did not model the observed house-size distribution explicitly as a log-normal, her model and the underlying process closely resemble the Gibrat's law described above. A (discrete) Poisson distribution with low rate can well approximate a (continuous) exponential, and a (discrete) binomial can approximate a (continuous) normal -- so a combination of the two (by multiplication of exponential and normal probabilities) will equally approximate a log-normal. Thus, simply by the social practice of patrilocal post-marital residence, a skewed house-size distribution would emerge spontaneously, without the presence of any additional structural inequality between community members. In this case, a log-normal distribution would be present in a settlement from its onset, and the skewness could or could not become more pronounced over time, depending on other factors like whether crop surplus would be distributed within the community or kept within households.

A last mechanism that is sometimes referred to in statistical literature as causing log-normal distributions, is that of random additive processes involving variables that by their nature cannot take on negative values, such as weights, heights or densities of physical entities [e.g. @johnson1994, p. 239]. While (two-parameter) log-normal distributions only can have positive values [@johnson1994, p. 208], normal distributions will often also have positive probabilities below zero, depending on $\mu$ and $\sigma$ values, and are then poor models of such quantities. A house, as an example, cannot have negative size, but will always be larger than some lower threshold above zero, and should thus also be more adequately modelled as log-normal than normal, even if the distribution looks symmetrical. This will in turn allow for more accurate estimates of other derived parameters, like the confidence limits for the coefficient of variation. However, in cases of symmetrical distributions where $\mu$ is much higher than $\sigma$, there is little practical reason to prefer a log-normal model over a normal one, as the probabilities of values below zero will be infinitesimally low.

\FloatBarrier

### Power-law distributions, preferential attachment and hierarchy {#power-law}

A variable $x$ is power-law distributed when its probability follows the power of itself with a fixed exponent $\alpha$, so that $p(x) \propto Cx^{-\alpha}$ [@clauset2009, p. 662]. Such distributions decrease very quickly as $x$ increases, and are thus highly skewed, but the probability never reaches 0 -- it is said to be *asymptotic* -- meaning that they are also very heavy-tailed (Figures \@ref(fig:04-PDF)a and \@ref(fig:04-cCDF)a). Furthermore, a power law can only take positive values, and there is always a minimal threshold $x_{min} > 0$ above which the function holds. The exponent $\alpha$, often termed *scaling exponent* or *scaling parameter*, will usually lie in the range $0 < \alpha < 3$, though values below 1 are considered rare special cases, when considering size or frequency distributions [@newman2005, pp. 331-2]. Low exponent values give more heavy-tailed distributions and *vice versa*, so power laws with a high scaling exponent (around 3 or above) are those that in practice will be more easily mistaken for other less skewed distributions like exponential or log-normal distributions (Figure \@ref(fig:04-pl). As with the previously discussed distribution models, the power law is also usually associated with a normalising constant (here denoted $C$), a factor that ensures that the area under the curve of the PDF sums to 1, and which here is defined as $(\alpha-1)x_{min}^{\alpha-1}$ [@clauset2009, pp. 664-5]. Applying the product rule of exponents, the power-law PDF or density function can be expressed as

```{=tex}
\begin{equation}
p(x) = \frac{\alpha-1}{x_{min}}(\frac{x}{x_{min}})^{-\alpha}.
(\#eq:power-law)
\end{equation}
```
In the cCDF or survival function, again the normalising constant is replaced with 1, but rather than left out it is written in the exponential form $(\frac{x}{x_{min}})^1$ which equals 1 when $x = x_{min}$, so that

```{=tex}
\begin{equation}
P(x) = (\frac{x}{x_{min}})^{-\alpha+1}
(\#eq:pareto)
\end{equation}
```
in the notation of @clauset2009, Eq. 2.6, equivalent to the more complex notation in @newman2005, Eq. 4.

As shown in Figures \@ref(fig:04-PDF)b and \@ref(fig:04-cCDF)b above, power-law PDFs and cCDFs hold the special property of appearing linear when plotted with logarithmic x and y axes (or equivalently when x and y values are log-transformed). This can be shown by log-transforming the simple functional form above, so that $log(p(x))=(-\alpha)log(x)+log(C)$, which is a linear model with y-intercept $log(C)$ and slope $-\alpha$. Power functions are in this way similar to exponential functions, with the difference that the variable $x$ is here in the base rather than in the exponent, causing the graph to be linear only when *both* axes are logarithmic (as opposed to one axis for exponential functions). Because of this property, the most common method for estimating $\alpha$ since the first formulation of the model and until recently [@clauset2009; @stumpf2012] was to plot the data on logarithmic axes, perform a least squares linear regression and estimate the slope (see Chapter \@ref(methods-distfit)).

The main reason for the great attention that has been given to power-law distributions over the years (see below), is its property of *scale invariance*, meaning that the distribution will appear the same no matter the scale in which it is being observed [see chapters 1 and 2 of @west2017, pp. 1-78 for a general non-technical introduction to scaling in science]. Figure \@ref(fig:04-pl) shows the relation between change in the variable being modelled ($x$) and its function value (here the PDF function) as a constant proportion of exponents (i.e. a linear relation between two vectors of logarithms). This means that for a power-law distributed variable, a change of scale (zooming in or out on the x-axis, changing order of magnitude no matter the base number) will always result in a proportional change of order of magnitude of the probability or frequency modelled on the y-axis. If $x$ represents a size related variable, and $y$ a frequency (probability in the PDF, rank in the CDF, or absolute frequency), this proportional relation in logarithms generates a self-similar hierarchy, where the same structure of sizes and frequencies is repeated across different scales (Figure \@ref(fig:04-hierarchy)). Thus, even though fractal structures are most often associated with geometric shapes in physical space -- like the shapes of plants, rivers or clouds -- they can be equally present in non-geometric variables, like income or wealth, magnitude of events, sizes of cities or nodes in networks. Such structures are, as with physical fractals, recognised by their abrupt skewness or high level of inequality, by their hierarchically distributive mode of functioning, and their statistical signature which is the power law. This connection between power-law distributions in general and fractals was first recognised and expanded upon by Mandelbrot [e.g. -@mandelbrot1997; short summary in -@mandelbrot1982, pp. 341-8].

(ref:04-pl) Examples of power-law distributions with different scaling exponent ($\alpha$) values on logarithmic axes, showing how this parameter reflects change over orders of magnitude. For a model with $\alpha = 3$ (blue), a decrease in probability p(x) of 3 orders of magnitude (powers of 10), e.g. from 0.1 to 0.0001 corresponds to an increase in the size of x of 1 order. For a model with $\alpha = 1.5$, the same decrease in probability corresponds to an increase in x of 2 orders of magnitude. The models appear linear in logarithmic space, but are in reality highly non-linear, as illustrated by the grid

```{r 04-pl, fig.cap="(ref:04-pl)"}
load("Results/fig04_pl.RData")
fig04_pl
```

(ref:04-hierarchy) A power-law distribution of sizes arranged in discrete levels, illustrating its characteristic scale invariance. From the largest element on top and downwards, sizes decrease while numbers (frequencies) increase, both exponentially but in opposite direction, generating a hierarchical fractal structure where the same shapes are recognised at different scales

```{r 04-hierarchy, fig.cap="(ref:04-hierarchy)"}
load("Results/fig04_hierarchy.RData")
fig04_hierarchy
```

As already mentioned, the power-law distribution was first formulated by Italian economist Vilfredo Pareto as a model of wealth inequality [-@pareto1896]. Over the 20^th^ century it saw an increasing number of applications in a wide variety of fields in natural and social sciences, modelling phenomena from magnitudes of earthquakes and sizes of moon craters to the intensity of wars, frequency of family names and citations of scientific papers [see @mitzenmacher2004; @newman2005 for detailed overviews]. The idea that the power law could be a suitable model for city sizes was seemingly first proposed in a short paper by @auerbach1913, though it is often attributed to @zipf1949, through which it has become known to and sometimes applied by archaeologists (see Section \@ref(distfit-archaeo)). The first influential attempt at an explanatory model of power-law distributions -- which at first were primarily descriptive, one of the main critiques against Pareto -- was made by Udny @yule1925, who seeked to explain the observed size distribution of genera by number of species. Yule's model [here following the notation in @newman2005, pp. 340-2] involves a set of $n$ genera consisting of a variable $k$ number of species each. During a discrete time step interval, a constant $m < n$ number of new species are added to the existing genera by speciation, so that some but not all genera will grow to $k+1$ in each time step. The probability for each genus of receiving a new species in a given time step is proportional to $k$ or the number of species it already includes, since a speciation is more likely to happen in an already large group of species than in a small group. Finally, for each time step one new species is sufficiently different to be considered a new genus on its own, so that $n$ increases linearly by 1 for each step. Under these conditions, over time $k$ will follow a heavy-tailed distribution with a power-law tail -- or strictly speaking a discrete version of it known as the Yule distribution [see @simon1955].

The precise mathematics involved in this model are quite complex for non-specialists, especially in the version presented by Yule before modern stochastic theory was developed, but even more recent and concise formulations will involve some level of calculus [@newman2005; @mitzenmacher2004, pp. 230-3]. The essential is however to note that the mechanism it describes -- now known as *the Yule process* -- is relatively simple and transferable to many natural as well as social settings. The heavy-tailed distribution of species will emerge even in the simplest scenario starting with one genus $n_1$ consisting of a single species ($k = 1$), and one added species to existing genera per time step ($m = 1$). Then the single genus will have probability $p(n_1) = 1$ of receiving the new species $m$ so that it gets $k = 2$, while the new genus $n_2$ starts at $k = 1$. In the next time step a new species will be given to one of these two genera, but $n_1$ has twice as high probability of receiving it as $n_2$ -- that is, probabilities are $2/3$ and $1/3$ respectively. In other words, the probability for any genus $n_{i}$ with $k_i$ species of winning the round (so to speak) and being attributed with the extra species, is given by $k_i/\sum{k}$, or the fraction of the total amount of species (in the given time step) that the genus already has. The main difference here with the above described Gibrat's law -- which also involves proportional growth -- is that in the Yule process the growth is not distributed evenly across the system. There is an additional selecting process that over time gives more to those that already have, thus according a disproportionate advantage to anyone who gets even the slightest advantage by chance from the offset -- which is why this is often referred to as a *rich-get-richer* process when applied to economics.

In fact, the Yule process having been recognised more or less independently within a number of disciplines over the years, it has come to be known by a plethora of different appellations, sometimes hiding the fact that they describe similar underlying mechanisms. The term *feedback loop* is derived from acoustics, describing the bothering situation when the sound from an on-stage monitor feeds into the microphones and back to the monitors, and so on, thus very quickly generating a sound so strong and high that it overturns the system. The sound going into the microphones is proportionally amplified to the volume of the sound coming out of the monitor, so that the stronger the input, the stronger the output and by consequence the next input, and so on. Another term, derived from attempts in sociology to explain the power-law distribution of citation frequencies of academic papers, and more generally the rewarding systems in academia, is the *Matthew effect*, alluding to a passage in the Gospel of Matthew (25:29): "*For unto every one that hath shall be given, and he shall have abundance; but from him that hath not shall be taken away even that which he hath*" [@merton1968]. A typical manifestation of such a process is when a prestigious research grant is attributed to a researcher based on academic merit, whereupon this in turn opens up numerous doors allowing for even higher career achievements, while the second best candidate, altough having arbitrarily close merits to the first before the grant, afterwards will have disproportional difficulties of following their pace in career development. For paper citations, one analogous explanation is that a paper that already has many citations, is more likely to be found in literature searches and be cited again than a similar paper with less citations [@newman2005, p. 341]. In other words, the probability of a new citation is proportional to the citations the paper (or its author) already has, but disproportional to the quality of the paper when compared to other papers of similar quality. The equivalent of the Yule process that has possibly received the most attention within social sciences since the turn of the millennium, is that of *preferential attachment*, first defined in a widely discussed study in network analysis by @barabási1999. They were the first to identify scale-free networks of links between websites in the then young World Wide Web, where a few sites were reported to have very high numbers of links to them, while the vast majority of sites only have very few. Their explanation of how such networks emerge -- strikingly similar to Yule's explanation of speciation in biological genera -- was that as new websites are made and the internet grows, these will tend to link (or preferentially attach) to existing websites that already have many links to them. It should be noted however that around 2010 there was a significant shift towards a more rigorous methodology that would be expected to accompany claims of power-law distributions in empirical data, leading to many previous claims being either weakened or fully rejected [@clauset2009; @stumpf2012; Chapter \@ref(methods-distfit)].

A range of other more or less related power-law generating mechanisms and processes have been proposed [@newman2005; @mitzenmacher2004]. Those that specifically relate to the magnitudes of events distributed in time may be of particular use in archaeology, though they have seemingly received little attention so far. Among these are approaches focussed on phase transitions and critical phenomena, like *self-organised criticality* or SOC [@bak1987; @bak2013]. This process models dynamical systems that continuously grow up until a certain critical point, at which it self-regulates downwards to stability through a collapse that is power-law distributed in size and frequency, before resuming the growth process again. The classic illustration of this model is that of a sand pile with a continuous addition of grains on top of it. Once the pile grows to its critical point where its slope becomes too steep, a sand avalanche is triggered and the pile stabilises again. The vast majorities of these avalanches are expected to be small in scale, but from time to time much larger avalanches appear. As the pile grows bigger, the critical point is also gradually raised, since a larger system can generally tolerate larger stresses before it needs to self-regulate. According to the theory behind such systems, the location of the critical point can be predicted with some accuracy, but the magnitude of the stabilising event that occurs when the system reaches this point, is impossible to predict beyond the power-law probability distribution it follows, i.e. that most events will be small but there is also a chance they will be several orders of magnitude bigger. The magnitude of the event is determined by the exact grain of sand in the pile that is the first to yield and thus triggering the chain reaction, but the scale of the potential consequences for the different grains varies greatly. The mechanism is said to be highly sensitive to initial conditions, or *chaotic*, similarly to how negligible initial differences in paper citations or website links over time can lead to disproportionally large differences, while it being virtually impossible to predict at the onset exactly *which* paper or website will come out on top [see @devaney2020 and @gleick1987 for general introductions to Chaos Theory].

There are of course many ways in which the phenomena described here can be transposed to prehistoric social settings (see Section \@ref(distfit-archaeo) below for examples of how this has been done). One important aspect of social processes that are known to generate power-law distributions, is that they often involve some sort of repeated competition, where actors who win once are more likely to win again later, thus cumulating their advantage. When the variable being modelled is house size, with the underlying assumption that this is a material reflection of wealth and/or power, and the distribution is recognised as a power law, the perhaps most straightforward interpretation is that of the emergence of a social hierarchy. The temporal process of this emergence could look something like the following. Let there be an initial population of households living within a social system that for this purpose can be described as egalitarian (that is, ignoring factors like gender inequality, random differences between households, heterarchical differentiations between kin groups etc.). If, for any reason, a competitive dominance process is triggered, a household that initially holds a random slight advance over others may gain a dominant position over them, as clan leader or chief household. The leader or leading house may then gain certain privileges and duties related to tribute and redistribution of goods, thus strengthening the ties of dominance between them and the group of dominated households. There is however an upper limit to how many households that can be effectively dominated by a single household, so this relation would be unlikely to define the entire population of households. On the other hand, the same process could play out throughout the population, generating a set of leaders or leading houses each dominating a similar group of households -- a situation that could be compared to the *house societies* described by Lévi-Strauss [-@lévi-strauss1982; Section \@ref(social-hierarchy)]. If the social, and by extension political, competition continues, it will play out between these house or clan leaders, so that the one that by chance has an arbitrarily small advantage over the others may gain dominance over all of them and thus also their dominated households, and so on up to state societies [e.g. @earle1997; @johnson1987]. As with the examples of the Yule process and preferential attachment discussed above, at any level of competition -- e.g. between chiefs to become king -- actors who find themselves further down the hierarchy, like clan leaders, have disproportionally lower chances of reaching that level, though in theory it is not impossible. Furthermore, the power-law relationship between leaders at different levels is recognised in that from any level to the next the number of leaders decreases exponentially while their dominance (the sum of households they each dominate) increases exponentially. The fact that most households remain at a bottom level with more random (*sensu* normal or log-normal) differences between them, does not change the fact that the leadership structure is hierarchical and power-law distributed -- meaning that the entire distribution of dominance in the population has a power-law *tail*, as expected for most empirical settings.

There are some critical issues with such a model of the emergence of social hierarchy. Firstly, if house sizes are only *symbolic* materialisations of dominance -- meaning that they do not in practice need to be large enough to actually fit all the people they dominate hierarchically -- the distribution of these house sizes would have a much shorter spread than what is expected in systems where there is a more concrete physical flow between elements, like oxygen in blood vessels. There is no natural proportion between decision-making power and square metres of floor space, and it seems likely that absolute house sizes in practice would be more constrained by building materials and techniques, which are also subject to change over time. Furthermore, several authors mention as a rule of thumb that a power-law tail should span at least two orders of magnitude in order to be accepted [e.g. @stumpf2012, p. 666; @brown2010, p. 53]. For a settlement where the bulk of houses are around 50 m^2^ on average this would imply that the largest house (in a hierarchy of largest houses) would need to be in the order of 5.000 m^2^ -- comparable to the Pantheon in Rome or the Hagia Sophia in Istanbul -- which, in addition to effectively excluding any prehistoric context, also seems as an unnecessary strict requirement for recognising hierarchy through house sizes. It is easily conceivable that smaller size differences could equally well be perceived as expressions of large social difference, e.g. if houses that are double the size of the average house (so 100 m^2^) belong to clan leaders and double that again (200 m^2^) to the chief. In that case, the size distribution would be more difficult to distinguish from a log-normal one, and if still interpreted as a power law, the scaling exponent ($\alpha$) should be expected to take on a value considerably higher that what is usually expected for natural systems.

Another issue in need of empirical investigation, is how large a social hierarchy needs to be before it can be recognised as a power law, or over how many levels it needs to span. The rule of thumb of an interval of two orders of magnitude could be interpreted as two levels of scale (the base number of 10 is of course entirely arbitrary), which in social terms could translate to at least a complex chiefdom [i.e. a system of commons, intermediate chiefs and a chief of chiefs, @johnson1987, #ADD PAGES]. A system with only a single leader or leading house per settlement as described above, could potentially not be recognised as hierarchical within a distribution fitting framework, even though it may have been lived and felt very much as a hierarchy for the people involved. A large social hierarchy will furthermore often span all settlements across a geographical area forming a settlement hierarchy, in which case studying house-size distributions within single settlements may be misleading. The overall scale of the hierarchy -- whether it concerns houses in a settlement or settlements in a regional polity -- is best understood from the largest element, and if this is missing from the data for whatever reason, interpretations in terms of social system will tend to underestimate the scale at hand. Luckily for archaeologists however, the largest settlements, as well as the largest houses, are usually the ones that are both best preserved and the easiest to discover, so that more often it is rather the lower end of the distribution that suffers from missing data, which has less importance to the interpretation of hierarchy (#REF?). It should also be kept in mind that in many -- maybe most -- hierarchical social contexts, the hierarchy may be highly organic and volatile, and there does not always need to be any discernible discrete levels. I would argue that even in social systems where discrete levels of hierarchy are clearly defined -- like in the feudal system of medieval Europe -- the hierarchy expressed through the material culture of the involved actors does not always need to tell the same story as the social hierarchy expressed through their titles of nobility.

It should also be noted that there are social situations that do not involve competition of power but that still can generate power-law size distributions. Most importantly is what can in a sense be seen as the opposite of the top-down hierarchy model described above, namely a bottom-up hierarchy, which structurally speaking can be very similar, or even exist as part of the former in a continuous dialectic tension [@furholt2020]. Anarchic or democratic societies can form complex (i.e. multi-scale) community structures based on clan or kinship structures [@hamilton2007; @haude2019, pp. 89-100], and if these structures are materialised in communal houses devoted to political assemblies or rituals, there is little reason to assume that the house-size distributions of such societies should be distinguishable from those of more top-down hierarchical contexts. An archetypical (pre-modern) example of such hierarchically structured democracies is the Iroquois League in the Great Lakes region during early European colonisation [e.g. @haude2019, pp. 127-31; @graeber2021, pp. 481-92], but contemporary Western bureaucratic democracies are also highly hierarchical [@delanda2006, pp. 67-91]. In any such cases, distribution fitting can allow for identifying the hierarchical structure, but not the actual type of government and to which extent its authority is based on bottom-up or top-down legitimacy. This qualitative aspect must still be investigated through other strands of evidence, like find inventories and symbolic representation within the large buildings. Building size itself will play a different role in democratic systems compared to autocratic ones, though the outcome will often be similar. In the former, communal buildings are by their very nature meant to be accessible and used by large parts of the community, and will therefore often need to be larger than common houses, while in the latter, leaders and high ranking people will be motivated to build houses for their private use that are larger than what they actually need, simply as a means to express their authority. In both cases the hierarchy is materialised in the house-size distribution, even though the absolute relationships between size and social importance may be culturally contingent. Exploring such specificities with hierarchical scaling in material culture has been one of the main motivations for this thesis.

In sum, the important characteristics of power-law generating systems, are that they are *complex*, meaning that they function or operate over several *scales* or orders of magnitude, which again implies that they exhibit *self-similarity* and can be described as *fractals* or *hierarchies*. Furthermore, they are rarely consciously planned, but rather tend to *emerge* spontaneously with growth, as a result of self-regulation and iterated responses to growth, like splitting or feedback that are proportional to size. Finally, even though such systems can be described as fully deterministic, they are also highly sensitive to initial conditions, i.e. they exhibit *chaos*, so that the magnitudes of single outcomes or trajectories can in practice be impossible to predict or forecast into the future, while the system as a whole can be well described, as well as explained backwards in time. Evidence of power-law distributed house sizes should be considered as a clear sign of a hierarchical social structure, though the exact nature of the structure -- and specifically whether it is autocratic or democratic, or something in-between -- needs to be argued from complementary evidence.

### Some variants of power-law distributions

A note must be made regarding terminology on power laws and some closely related distributions. Contrarily to normal, log-normal and exponential distributions, power-law distributions are not defined in a uniform way across all the disciplines where they are applied. Despite all claims of ubiquity, they remain special cases in many contexts, are not systematically taught in basic introductions to mathematics or statistics, and their broadened understanding seems to have suffered from long-standing discipline-specific traditions of defining them. Archaeologists who borrow theory and method from different fields therefore run the risk of talking past each other and not seeing the bigger picture of common phenomena described in different ways. Two alternative and more or less parallel ways of describing power-law distributions run under the names *Pareto* and *Zipf distributions*, after the first researchers who became known to define them under their specific parametrisations.

The so-called Pareto distributions form a group of distribution types with varying numbers of parameters, and are most often associated with applications in economics and finance [Pareto was an economist and his major works were on wealth and income distributions, see Chapter 20 in @johnson1994; @pareto1896]. Comparing with the power-law definitions given above (Equations \@ref(eq:power-law) and \@ref(eq:pareto)), it is important to note that Pareto distributions are defined as survival functions, i.e. cCDFs (complementary cumulative distribution functions) or the proportion of the distribution that is *higher* than a sample $X$. The scaling exponent for a Pareto distribution [@johnson1994 confusingly note this as $a$, p. 573, but here I prefer to use $\beta$ for clarity] thus has the value of the power-law exponent $\alpha-1$, or in negative values $-\beta = -\alpha +1$, meaning that Pareto plots have a less steep slope than their power-law counterparts. Accompanying their use in applied economics is the appellation of the "80-20 rule", a rule of thumb stating e.g. that in a company 80% of sales will typically go to 20% of clients as a result of the very skewed distribution. This specific rule corresponds to a power-law distribution where $\alpha \approx 2.1$ or $\beta \approx 1.1$ [@newman2005, p. 334].

Zipf law distributions, often referred to as "rank-frequency" or "rank-size" plots or rules, are most associated with linguistics, due to Zipf's work showing that word counts (as well as city sizes) tend to follow power laws [@zipf1949; @arshad2018]. Only some minor plotting conventions differ Zipf plots from Pareto plots, but again this influences systematically the value of the calculated scaling exponent. While Zipf distributions are also cumulative, they are not normalised to 1, meaning that absolute size ranks are plotted instead of probabilities. Furthermore, these are plotted on the x axis rather than the y axis, with the implication that the distribution is *discrete* rather than continuous. The y axis (the actual variable being measured) can in principle be discrete or continuous, but the most commonly cited cases are discrete, i.e. count data, like the number of inhabitants in a city or the number of times (frequency) a word appears in a text. The scaling exponent of a Zipf law [or a Zipf exponent, denoted $q$ e.g. in @arshad2018, p. 78] can thus be expressed as

```{=tex}
\begin{equation}
q = 1/\beta = 1/(\alpha-1).
(\#eq:zipf-exponent)
\end{equation}
```
The fact that there does not seem to be any consensus on which character to use for the different parameters is an additional difficulty to handle when comparing studies that apply these different approaches. In his extensive review of these three separate research traditions, Mark Newman (a physicist; the power law is the most common variant within the natural sciences) admits that this inconsistent nomenclature "causes much confusion in the literature" [-@newman2005, p. 327], which I will claim is a polite understatement. For clarity, throughout this thesis I only refer to the scaling parameter of the power law as defined in Equation \@ref(eq:power-law), which I denote $\alpha$, no matter the graphical representation of the distribution (most of the plots here are cCDF plots, and could thus be qualified as Pareto plots). The value of $\alpha$ is furthermore only calculated directly on the data using the maximum likelihood method described in Chapter \@ref(methods-distfit), and is thus not dependent on the type of plot chosen for graphical illustration.

As already mentioned, regular power laws are asymptotic so that as $x$ approaches infinity, $y$ goes arbitrarily close to $0$ without ever reaching it. This in itself acts as a limit to its usefulness for modelling most real-world phenomena which are of finite size, even though the power law may be a good approximation over some range of the system. In other words, in a power-law model of house sizes, no matter the value of $\alpha$, the probability of a house measuring a trillion square metres is certainly negligible, but still technically above $0$. Several solutions have been proposed to this problem, allowing the model to end somewhere at $x < \infty$. Among the main candidates are the *power law with exponential cutoff*, the *stretched exponential* and the *parabolic fractal* distributions. The first one is -- as the name indicates -- a power law with an additional exponential factor, which turns the function increasingly (exponentially) downwards as $x$ gets higher [see @clauset2009, p. 664 for precise definition]. The second is a type of exponential distribution, similar to the one described above (Equation \@ref(eq:exponential)), but where the $x$ exponent is raised to a power constant $0 < c <1$, giving a survival function $P(x) = e^{-(x/x_{min})^c}$, resulting in a tail which is heavier than that of a regular exponential distribution but thinner than that of a regular power law [@laherrere1998; @clauset2009, p. 664]. This survival function or cCDF of a stretched expoential is furthermore equivalent to that of the *Weibull distribution*, a well studied model type that is widely implemented in statistical software, making it easier to include in analyses compared to the more piecewise power law with exponential cutoff [comp. @johnson1994, p. 629 ff.].

Finally, the so-called parabolic fractal distribution has been proposed as a quadratic function model on log-transformed values of size to rank (Zipf plot), to account for cases where the distribution follows a parabola of constant curvature on a log-log plot [@laherrere2000]. Jean Laherrère -- working for a large oil company modelling the size distribution of oil reserves -- noted how studies that claim to show power-law distributions in empirical data often tend to explain away the curvature of the distribution in log-log plots, e.g. by focussing on shorter ranges of the tail, or by constructing composite distribution models. While the log-normal model forms a parabola on log-log plots of its PDF (Figure \@ref(fig:04-PDF)), none of the above discussed models have this characteristic for their survival functions. A quadratic function is a simple polynomial including a squared term of the variable, of the type $f(x) = ax^2 + bx + c$. The use of the term *fractal* here relates to the shape of the distribution being evaluated with log-log transformations, so that the function is interpreted as a power law with an additional squared factor, $a$ and $b$ both representing scaling exponents. With quadratic functions, the coefficient to the squared term (here $a$) determines the curvature of the model, while $c$ has the role of y-intercept as with linear models. The model can furthermore be interpreted as a power law where the scaling exponent $\alpha$ is continuously increased with higher $x$, in other words where $\alpha$ corresponds to the derivative of the parabola at any given $x$, forming a continuous spectrum characteristic of multifractals [see e.g. @harte2001]. Though this model seems to have much potential both for modelling and explaining self-affine phenomena within finite systems, it is relatively recent and not easily applicable without specialist knowledge in statistics and programming, and is not further included in this thesis. Future studies would be warranted, e.g. applying the analyses proposed in @laherrere2000 and @laherrere1998 for modelling sizes and frequencies of undiscovered and/or lost archaeological sites within given regions.

## Fitting heavy-tailed distributions in archaeology {#distfit-archaeo}

The distribution fitting approach adopted in this thesis is inspired by a relatively small number of previous studies in archaeology. Though the relationship between households and house sizes had already been thoroughly studied more generally by archaeologists and anthropologists since at least the 1970s [see Section \@ref(house-sizes-ethno)], the earliest quantitative study of prehistoric house-size distributions accompanied with theoretical arguments for interpreting power laws as signatures of multi-level social complexity, was perhaps a study by Herbert Maschner and Alexander Bentley published 20 years ago [@maschner2003]. They presented a well-argued case for hierarchical scaling between households in a study area on the Alaska Peninsula, apparently discernible in several periods of the region's prehistory, and explained this as the result of an elite emerging from various (though unidentified) competitive socio-economic practices. The study could today be criticised for methodological shortcomings -- the authors relied on least squares fitting on log-transformed binned data, they did not systematically propose more than one model for the data, nor propose any quantitative way of selecting the best fits, and their data sets seem to have been severely time-averaged, rendering any claims of hierarchical scaling between households potentially meaningless. All of these issues have been thoroughly addressed in subsequent studies (see Chapter \@ref(methods-distfit)). The general analytical procedure and rationale however, remains highly innovative and would merit far more attention than it has received. In my view, this study represented the first turn beyond the study of size *averages* towards a theoretically informed study of size *distributions* in household archaeology (#Check this sentence again after writing the top of the chapter).

This approach was further explored by @brown2012 and expanded upon by @strawinska-zanko2018, who identified a shift from less heavy-tailed distributions (exponential) to power-law distributions (Pareto) of house sizes in the Maya region approximately coinciding with the pre-Classic/Classic transition, which is traditionally considered the onset of state-level organisation. Following political scientist Manus @midlarsky1999, @strawinska-zanko2018 argued that this transition to a power-law distribution of wealth (proxied through house sizes) could be explained as resulting from increasing competition for agricultural land ownership following population growth. Furthermore, they identified a trend towards more pronounced inequality, both through lower $\alpha$ values and higher Gini indices, until the end of the Classic period, with an abrupt shift to more equal distribution in the post-Classic (higher $\alpha$ and lower Gini). Despite the inclusion of only four settlements in this case study, it remains highly interesting since the overall economic and demographic development of the Classic Maya is very well documented from a range of other approaches and intensive research. The study also contributed with detailed discussion of methodological issues, comparing the performance of different procedures. In a study by @crabtree2017 a similar analysis was performed on data from the Mesa Verde region in the American South-West over the 7^th^ to 13^th^ centuries CE. This cultural context is also very extensively documented, and holds the additional advantage of a fine-grained temporal sequence supported by dendrochronology, allowing for detailed analyses of near-coeval features. With the same motivation of using distribution models as proxies for underlying generative mechanisms, the authors systematically compared best fit log-normal and power-law models of settlement sizes as well as the sizes of *kivas* -- a special category of communal ritual structures -- across the study area. Though the results were not entirely unanimous, both indicators pointed towards a settlement hierarchy in the Pueblo II phase from ca. 1030-1140, centred around Chaco Canyon receiving tribute from surrounding areas, which again is coherent with the current understanding of the period based on other strands of evidence. Furthermore, they implemented preferential attachment mechanisms into an agent-based model of the regional socio-demographic development accordingly, largely reproducing the observed temporal patterns. This also seems to have been the first archaeological study consistently performing distribution fitting by maximum likelihood estimation rather than least squares, and comparing the fit of different models quantitatively. For this they used the same R package *poweRlaw* that is used here in the following chapters [@gillespie2015].

The analysis of the size distribution of special communal structures across a region, as was done by @crabtree2017, is in some sense a bridge between analyses of house-size and settlement-size distributions. #GROVE2011 from here.

-   Zipf law in archaeology, then A law (distribution) is not a law (of nature), see @grove2011 for review of the long-lasting confusion in archaeology (e.g. @hodder1979), also "rank-size rule"

-   Zipf law and Settlement Scaling theory, @bettencourt2021, @gomez-lievano2012, @lobo2020, @duffy2015 Connection with Central Place Theory, e.g. @müller-scheeßel2007, @chen2011. Why I'm not doing settlement scaling in this study.

-   SOC in archaeology, see [@bentley2001; @diachenko2022; @zhukov2016].

-   Not fitting distributions in archaeology, just assuming they are heavy-tailed, or avoiding the question: ex. @brink2013 (could include lots more!)

END chapter
