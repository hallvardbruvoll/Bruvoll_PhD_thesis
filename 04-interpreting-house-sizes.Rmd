# (PART) Size distributions {.unnumbered}

# House sizes and social meaning {#house-sizes-theory}

## Possible reasons for house-size difference

In archaeology there are two recurrent and seemingly contradicting assumptions underlying interpretations of house-size differences within a society. In studies where the goal is to provide population size estimates, this is often calculated from total living area, adding together the areas of the houses in question, and multiplying by a mean value of surface per inhabitant (**give examples**). This mean value is generally obtained from multiple ethnographic parallels. With this assumption -- that every inhabitant requires a similar amount of living space (**mention actual values from literature**) -- the size of a house effectively reflects its number of inhabitants. The second assumption, which is more frequently seen in studies focussing on wealth inequality and social stratification, is that house-size differences are expressions of differences in some sort of wealth or power (**give examples**). In this view, a larger house would have belonged to a wealthier household, capable of procuring more raw materials and activating a larger labour force for its construction and maintenance, in which case there would be significant disparities in living space per individual. Even though there is not necessarily any contradiction between these two interpretations from an anthropological point of view, most archaeologists seem to be unable to consider both possibilities simultaneously. From a methodological point of view, each of these assumptions will tend to mask our ability to see traces in the archaeological data relating to the other assumption -- that is, with existing methods we cannot convincingly provide estimations of both population size and level of social inequality from the same data, even though house sizes frequently form the basis of both argumentations. Far from proposing a solution to this issue, my argument here is that a variety of social institutions known from ethnography and historical sources can explain some level of correlation between the two variables. Dowry and bride price are geographically and temporally widespread practices that link number of offspring (daughters, sons or both, depending on cultural context) with wealth. Clan leaders may draw upon kinship ties and dependencies in order to obtain the workforce needed to construct a larger house (**citation needed**). Nevertheless, if the notion of wealth is at all to be applied meaningfully to non-capitalistic societies, it should designate cases where there is significant and persistent material disparities between members of a population, and not simply point to different household sizes. House sizes could thus potentially reflect a somewhat more complex culture-specific interplay between household size and wealth, meaning that for a given house size, one could assume a range of possible values of the two parameters (**insert fig/illustration**). This relationship between household size, wealth and house size should be studied more in detail empirically through the available ethnographic data, rather than reducing its complexity to a mean surface area per inhabitant for the entire population. Such a study, however, lies beyond the scope of the present thesis. Here I will largely leave aside the question of population and household size, focussing on distribution types of house size data, arguing that the most unequal distribution type considered here -- the power-law distribution -- is unlikely to emerge only from random differences in household size and standard marital patterns, favouring thus interpretations relating to wealth and/or power differences.

Even though the goal here is not to investigate household sizes but to focus on the material aspect of house-size distributions, some fundamental issues of terminology should be addressed. The use of the word *house* (in the wider material sense rather than the Lévi-Straussian sense, see Section \@ref(social-hierarchy)) is indicative of an underlying assumption that the building in question was in use primarily for domestic purposes -- essentially, a fixed architectural unit where someone would spend their nights at least most of their time, and in many cases also cook and eat their main meal during the day -- though proving this directly is not always straightforward in archaeology. For the cultural contexts discussed here -- the Linear Pottery and Trypillia groups in the Neolithic-Chalcolithic -- there is however little evidence for buildings with specific non-domestic (e.g. economic, religious or administrative) purposes, with the probable exception of the so-called mega-structures or assembly houses in the Trypillia mega-sites, which are discussed more in detail below. This lack of evidence does not imply that there was no specialised economic, religious or administrative activity in these societies, as ethnography and history clearly shows that such activity must reach a certain degree of specialisation before it materialises in distinct buildings devoted exclusively for these functions. Artisans, shamans and chiefs could be specialised to some extent but still perform their activity at their domestic home or more diffusely outdoors or without any fixed location [e.g. @costin1991, p. 25]. It is in any case of common usage to speak of houses when discussing architectural units in Neolithic Europe and other prehistoric contexts, maybe because of a lack of a better generic word, but this usage should not prevent archaeologists from recognising other non-domestic functions of buildings whenever there is evidence for it. The *household* is furthermore the designation of all the people, genealogically related or not, usually living under the same roof or within the same architectural unit, constituting a functional whole economically and socially, and potentially including more than a single family unit, as well as servants or slaves, depending on the context.

Kinship studies within anthropology have shown over the last decades that in most societies, contrarily to common misconception, kin affiliation is *not* simply a matter of biological relatedness. In an attempt of grouping together all possible justifications for kinship ties, Marshall Sahlins [@sahlins2013] defined kinship as "mutuality of being" -- that some real or imagined substance is shared, and that this substance is not necessarily genes, as is mostly the case in modern Western societies (with adoption as the main exception). A variety of non-genetic foundations of kinship are widely accepted in different cultures, like sharing of name, time or place of birth or childhood, food source, shared experiences, blood ties, and so on. At the same time, archaeology as a whole has arguably been very slow in taking this diversity of kinship configurations into account, far too often taking for granted the modern Western (especially post-war 20th century) ideal of patrilineal nuclear families as the default configuration for all of human history [e.g. @ensor2021]. That being said, anthropological kinship studies such as that represented by Sahlins typically shows little concern with material culture, and is mostly silent on the question of who -- at the end of the day, quite literally -- sleeps under the same roof, making it hard to interpret domestic architecture in terms of kinship structure and social organisation. The recent comeback of kinship studies in archaeology has been far more focussed on linking isotope and aDNA data with social structures, largely fuelled by the rapid developments of the related methodologies [e.g. @carpenter2022 ; ]

-   Kinship and households: who lives in a house? @sahlins2013, @ensor2013, @ensor2017.. archaeology: @carpenter2022, @madella2013, @joyce2000, @blanton1994, @carsten1995, @hofmann2013, @wilk1982

-   Do clan leaders have bigger houses? check @haude2019, @kahn2021, @bradley2013, @wilk1983.

Persons who possess enough food resources to create obligations through gifts and feasting... What is wealth? House construction as communal activity @goodale2021

P. Květina and J. Řídký point out both architecture (construction, size, orientation) and settlement layout as possible distinctive features between Big Men (achievement-based) and Chief societies, arguing that the former type may be recognised by a dispersed intra-settlement layout combined with uniform architecture, while the latter type would tend towards more regular settlement layout and more marked differences in architecture [! @kvetina2019, p. 13]. EXPAND UPON THIS.

Discuss some archaeo references

-   Schiesberg 2010 2016, go through refs in Zotero, family size and houses for the LBK

Functional difference:

-   Ethnography of initiation houses, communal/assembly houses, ritual houses, including @barley2011, @godelier1986, @wilk1983, @fraser1968, @haude2019

-   Caveats: building materials and constraints, climate (heating), mobility, multi-floored, see @porcic2012, residence pattern and floor area in @porcic2010.

## Interpreting distribution types and their underlying mechanisms {#distributions}

The main question in this part of the thesis is on the nature of house-size differences in the studied contexts, and to explain them either as being due to random fluctuations or as a material expression of a more structural inequality, or something in-between. In statistical terms, this is a question of distributions. A distribution is a mathematical model of the spread of data along a variable or axis, and it can be modelled on empirical data as a succinct description, or used to predict unobserved data (e.g. future developments, or, in the case of archaeology, data that is lost to taphonomy). In theory, there is an infinite number of possible distributions, as there is no limit to how many parameters one can include to fit the data. It is however generally considered good practice in statistics to limit the number of parameters and identify the simplest possible model that gives a good fit, since a larger number of parameters can often lead to a better fit, but at the same time be harder to explain in terms of underlying mechanisms. Adding many parameters only to achieve a marginally better fit is referred to as *overfitting* (#ref?), and for most real-world contexts there is a limited number of model types that can be considered reasonable candidates. In cases where we are interested in inequality or uneven distributions of data (so-called *skewed* distributions), the most likely distribution models are those that can be described as *heavy-tailed*, meaning that on typical graphical representations (like density/PDF plots or histograms) they will show a characteristic stretch of some of the data towards the right end of the x-axis, while most of the data remains on the left side (Figure #ADD illustration). The opposite orientation is also possible in theory, in which case the distribution can be referred to as left-tailed. It is important to keep in mind however, that not all distributions are heavy-tailed, and that other distributions also model the spread of data across a variable, but result from very different underlying mechanisms. Identifying the most likely distribution model for a data series is therefore crucial for understanding how the data could have been generated. And though it is true that one model type can have multiple different explanations -- in an archaeological setting for example, many different social behaviours can lead to the same material outcome, an issue known as *equifinality* -- excluding one or more model types for the observed data can help limiting the number of plausible interpretations considerably.

Distributions can be represented graphically in a number of ways, most commonly in a coordinate system with the variable on the x-axis and its density, or *probability density function* (PDF), on the y-axis (#Fig again). Often, and in many of the sources cited in this part of the thesis [e.g. @clauset2009], the PDF is denoted by $p(x)$, reading as *the probability of x*, or simply $f(x)$ (*the function of x*). It gives the probability for a drawn sample ($X$) of falling within a given arbitrary range of the distribution, written $Pr(x \le X < x+dx)$. By definition, the area between the PDF curve and the x-axis sums to 1. For reasons that are further discussed in Chapter \@ref(methods-distfit), heavy-tailed distributions, and power laws in particular, are instead often represented with their *cumulative distribution function* (CDF), which is the integral of the PDF (inversely the PDF is the derivative of the CDF). Similarly to the PDF, the CDF is often denoted as $P(x)$, depending on disciplinary tradition. It indicates the probability of a random sample value being equal to or lower than the function value, or $Pr(X \le x)$. Furthermore, the specific version of the CDF used for plotting heavy-tailed distributions, is the *complementary* or upper tail CDF (sometimes referred to as the *survival function*, or denoted cCDF), which is $1-CDF$ or $P(X \ge x)$, indicating the probability that a random sample is higher than the function value. Both axes on such cCDF plots are traditionally set on logarithmic scales, usually $\log_{10}$ for readability. To avoid confusion, in this thesis I refer to PDF and cCDF for density and distribution functions respectively (except in equations, where I use $p(x)$ and $P(x)$ respectively), and all PDFs are plotted on linear scales and cCDFs on $\log_{10}$ scales, unless otherwise stated. Apart from scales on plots, whenever I refer to logarithms (i.e. in calculations), I imply natural logarithms, that is $\ln$ or $\log_e$ where the base number $e \approx 2.718$.

In the following, I present briefly the main distribution types that will be discussed further in the following chapters, with special focus on the *power-law distribution*, which is the model type associated with fractals and structural hierarchy. All of these distributions are modelled on continuous univariate data series [see @johnson1994 for more detailed presentations]. Even though their mathematical definitions may seem complicated to non-initiated readers, most of the distribution types discussed in this thesis are readily implemented in standard statistical software, including Microsoft Excel, allowing for a more straight-forward use of them. For this thesis, I used base *R* functions for calculating PDFs and cCDFs, and for random number generation for normal, log-normal and exponential distributions [@R2023], and equivalent functions from the *poweRlaw* package for power-law and Weibull/stretched exponential distributions [@gillespie2015].

### Normal distributions and the Central Limit Theorem

One of the distribution models that are the most commonly referred to and well-known, is the *normal distribution*, also known as the "Bell Curve" due to the characteristic bell shape of its PDF (#Fig.), or Gaussian after mathematician C.F. Gauss who contributed to its exploration in the early 19^th^ century. Its characterising parameters are the *mean* (denoted $\mu$, *mu*) and *standard deviation* ($\sigma$, *sigma*), and the PDF is defined mathematically as
\begin{equation}
p(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-(x-\mu)^2/2\sigma^2}
(\#eq:normal)
\end{equation}
where $e$ (*Euler's number*) and $\pi$ (*pi*) are mathematical constants [@johnson1994, pp. 80-8]. This distribution is centred around its mean, with dwindling amounts of data spread outwards to either side. The mean -- commonly known and widely used in daily speech -- is the sum of all observations divided by number of observations, or
\begin{equation}
\mu = (\sum_{i=1}^nx_i)\,\frac{1}{n}.
(\#eq:mu)
\end{equation}
The spread of the data from the mean is defined by the standard deviation, which is the square root of the mean of all squared deviations from the overall mean, or
\begin{equation}
\sigma = \sqrt{(\sum_{i=1}^n(x_i-\mu)^2)\,\frac{1}{n}}.
(\#eq:sigma)
\end{equation}
Mathematically, the square of the standard deviation, $\sigma^2$, or *variance*, is simpler, but since it is also less intuitive, I refer here to the former whenever possible. The standard deviation can be thought of as the mean of all deviations, positive or negative, from the overall mean.

The great importance the normal distribution has to a wide range of phenomena is explained through the *Central Limit Theorem* (CLT), according to which the sum of random variables tends to a normal distribution as the number of variables increases towards infinity, under certain conditions [@johnson1994, pp. 85-8]. More specifically, if $X_1, X_2, \dots, X_n$ are independently drawn and identically distributed (condition referred to as *i.i.d*) random variables or samples, their sum will be normally distributed in the limit as $n$ tends to infinity, or
\begin{equation}
\lim\limits_{n \to \infty} \sum_{i=1}^n X_i = \mathcal{N}.
(\#eq:CLT)
\end{equation}
The sum may be standardized in some way to avoid infinite numbers, but in practice $n$ will always be limited, so the resulting distribution will always also be approximately normal at best. The original distribution of the random variables (i.e. that of the *population*) does not need to be normal for the theorem to hold, but can be any distribution as long as its variance is finite. The normal distribution (referred to as the *sample distribution* in this context) is related to the population distribution of the random variables (samples), in that the mean of the population equals the mean of the sample distribution divided by $n$, and the standard deviation of the sample distribution, referred to as the standard error $s$, equals the standard deviation of the population divided by the square root of $n$, or $s = \frac{\sigma}{\sqrt{n}}$, meaning that $s$ is reduced with higher $n$ (i.e. larger sample size). A common variant of the CLT is that the distribution of sample means, rather than sums, are normally distributed, in which case the mean of the sample distribution equals the mean of the population distribution.

In more laypersons' terms, the total weight of a box of strawberries is the sum of the weights of the strawberries it contains (subtract the weight of the box itself, and assume the same number of berries per box for the sake of argument). In the packaging facility, a large population of strawberries are continuously distributed randomly into boxes of the same size. The berries in the factory (the population) have weights that follow some distribution with limited variance -- some are bigger than others, but there are upper and lower limits to how much a strawberry can weigh -- and all the berries in the boxes are drawn from this same population (so identically distributed). New berries are shipped to the facility all the time, and berries are not sorted according to size but randomly mixed, so if one box by chance gets filled with only very large berries, that does not necessarily affect the weight of subsequent boxes (each box is an independently drawn sample $X$ of $n$ berries). Under these circumstances, the weight of boxes of $n$ berries (i.e. the sum of the berries' weights, the sample distribution) will be normally distributed by the Central Limit Theorem.

Some details here are crucial: the distribution of the strawberry weights themselves will, with higher $n$, tend towards that of the population, which is not necessarily normal. The theorem is only valid for summary measures like sums or means, and not for the observations directly. If the strawberries at the facility are mostly large (heavy) and with only smaller proportions of small berries, this skewed distribution will be reflected in strawberry boxes of a certain size ($n$), but since the influence of this distribution is the same on all boxes of the same size, their overall weights (or mean weights) will be normally distributed. Furthermore, if we draw some boxes from one producer and some from another producer who has significantly larger or smaller berries, the samples are then drawn from different populations and are thus not identically distributed, and the sample distribution will not necessarily be normal (colloquially referred to as "comparing apples and oranges"). Similarly, if there is an overall trend of boxes becoming heavier over the season, then samples from across the season will not be normally distributed. If however, we compare the mean box weights from multiple seasons, these will again be normally distributed, unless there is also a multi-year trend of strawberries becoming larger or smaller. In other words, the i.i.d. condition for the CLT to hold, means that the samples are drawn at random, with no important underlying trends. As a side note, there is a substantial body of research on the conditions under which normal distributions can emerge *without* the i.i.d. condition being met [see @johnson1994, pp. 87-8 for details and further references].

Also important to note is that the number of samples $X$ (boxes in the example above) does not matter to the shape of the sample distribution, other than to the resolution of the curve or binwidth of the histogram when plotting, and the weight of a single sample can be modelled as a probability following a distribution. The normal curve of the sample distribution can be entirely defined by the population $\mu$ and $\sigma$, and sample size $n$. In many practical settings however, the parameter values of the population distribution (and even its type of distribution) are unknown, in which case more samples are needed in order to model a sample distribution and from there infer the parameter values of the overall population. This is the case when statistical tests like Student's t-test or ANOVA are applied for examining the relations between samples and populations.

In other cases, $n$ (sample size) can be unknown, but assumed to be approximately the same for all $X$ (samples), like in the somewhat more abstract cases where each of the $n$ variables contained in $X$ are of different nature -- or stated otherwise, when the size of $X$ is the sum of many different and independent causes. In the case of a normal house-size distribution within a given cultural setting (like a village), one can assume that the same number of causes affect the size each house takes when constructed, but to varying degrees (colloquially we often say *factors* for such causes, though in this setting one should strictly speaking prefer *terms*, *summands* or *addends*, since they add up rather than multiply). Say that house size in a given context results from the cumulative effect of household size, inherited wealth, soil quality, exposure to sun, wind or flooding, artisan specialisation, raw material availability in the year of construction and many more variables. Each of these may have separate probability distributions -- e.g. the wealth distribution may be heavy-tailed while household size may be normal and symmetric -- but as long as the overall population, so to speak, of contributing causes is distributed in the same way to all households, and that none of the variables dominates the effect of the others, and the value of each variable is independent from the values of the other variables, their sums expressed in house sizes should be normally distributed by the CLT. Such a distribution is then an expression of *random difference*, and in the case of house sizes, there would be no particular reason as to why a few houses would be bigger than all the others, just as a few houses would be smaller, while most would be centred around the mean size. But again, if samples are drawn from *different* populations, e.g. houses from different villages or cultural contexts -- where the probability distributions of the underlying variables are categorically different -- the i.i.d. condition is not met, and the distribution of house sizes is unlikely to be normal. Likewise if the population contains grades, i.e. is grouped, or where the different variables are correlated between them, *e.g.* if wealthier households are also larger households and have more access to raw material and better quality soils, and so on, their house sizes may also become disproportionately large and deviate from normal expectations.

### Exponential distributions and constant rates of growth and decay

The exponential distribution is -- next to the normal -- one of the most widely applicable distribution models. In its simplest form, it is a function of a positive random variable $x$ where some base number (usually $e \approx 2.718$, for compound continuous growth) is raised to the power of $-x$, in other words when $x$ has the probability density $$p(x) = e^{-x}.$$

This simple form is called the *standard* exponential distribution, and in most practical applications there will also be a *rate* parameter $\lambda$ (*lambda*), so that the density function becomes $$p(x) = \lambda e^{-\lambda x}.$$

In the case of the standard version, $\lambda = 1$ and can be left out. The negative rate in the exponent is the actual rate that determines the shape of the distribution, whereas the rate multiplier to the base is a *normalising constant* which assures that the area under the curve adds up to 1, and thus that the values shown on the y-axis are probabilities. This constant can be though of as the y-intercept of a linear model, since at $x = 0$ the function gives $\lambda e^0 = \lambda\,1$. This is seen more clearly if we take the logarithm of the exponential density function, $p(log(x)) = log(e)\,(-\lambda x) + \lambda = -\lambda x+\lambda$, which is a linear model with slope $-\lambda$ and $\lambda$ as y-intercept. As a rule of thumb, an exponential distribution can thus also be recognised as a straight line on a plot with one linear and one logarithmic axis. A wide variety of more complex forms have been formulated, and the distribution type is furthermore generalisable to both the Gamma and Weibull distributions [@johnson1994, pp. 494-9]. Note that the simple form presented here may also have more complex notations in specialised statistical literature, *cf*. Eq. 19.1 in @johnson1994, which is equivalent to the density function above given that $\sigma^{-1} = \lambda$ and $\theta = 0$ [see also @johnson1994, pp. 522-3; @clauset2009, p. 664].

Exponential distributions have the highest probability (so the most data) at low values of $x$, and ever lower probabilities towards the right end of the curve (#Fig.), with the rate of diminuendo determined by $\lambda$ -- the higher the rate value the steeper the curve falls off from left to right, and inversely low $\lambda$ values give more heavy-tailed distributions. The type of setting which is most commonly modelled as an exponential distribution, is that of "events recurring at random in time" [@johnson1994, p.494]. If $x$ represents the the duration in time between events (or duration of single events) that occur continuously and independently from each other, with a constant average rate of $\lambda$ events per unit of time, it can be modelled as an exponential distribution. An important feature of the underlying process (a so-called Poisson point process), is that it is memoryless, meaning that the duration between events $X_1$ and $X_2$ does in no way affect the duration between $X_2$ and $X_3$, as all durations are drawn independently from the distribution with the same average rate $\lambda$. The example of such a process that may be the most familiar to archaeologists, is the radioactive decay of the ^14^C isotope with its average decay rate $\lambda \approx 0.00012$ or 0.012% per year. With a rate that low and using years as the time unit, it is more useful and intuitive to work with the *half-life* measure, or the time it will take on average for the initial quantity to be halved, which for ^14^C is about 5730 years. The half-life (the median of the distribution) is given by $\log 2/\lambda$, solving for $x$ in the cCDF function $e^{-\lambda x} = 1/2$, where the normalising constant in the PDF is replaced with 1 (and therefore omitted in multiplication) for the initial quantity or y-intercept. If we replace $1/2$ with the remaining proportion of ^14^C in the organic material of an ancient artefact, compared to the expected amount in the same material when alive, we can use the same equation to solve for the approximate year when the organic material died, which is the principle behind radiocarbon dating. In a radioactive decay process, a total amount of individual unstable isotopes are present from the start (*e.g*. the death of an organic material), and their individual lifetimes until decay are exponentially distributed. As an additional metric of exponential random variables, the mean $\mu$ or *expected value* is given as the reciprocal of $\lambda$, so that $\mu = 1/\lambda$ and $\lambda = 1/\mu$. The mean is larger than the half-life, and for ^14^C decay, this corresponds to $\mu \approx 1/0.00012 \approx 8267$ years.

The exponential distribution can also model many processes that are closer to an everyday human scale than ^14^C decay. Expanding from the example used for normal distributions, let $\lambda$ be the average risk for a strawberry of being harvested within a week $x$. As the weeks go by (as $x$ increases), the cumulative risk of being picked grows exponentially, so that there are very few berries that are older than a few weeks, and the mean age of the berries in the field is $1/\lambda$. The berries are furthermore picked by a harvesting machine that is unable to aim for a certain size category, and the berries grow linearly (which may be a rather poor approximation, but for the sake of argument). The harvested berries are also continuously replaced by new berries which start growing at the same pace, so the field is always renewed. Under these circumstances, the lifetime of a strawberry during which it grows is exponentially distributed, and the probability of surviving $x$ weeks follows the density function above. The example illustrates how exponential functions model repeated multiplication or multiplicative processes, since the $x$ in the exponent means "multiplied $x$ number of times". The rate $\lambda$ (technically $e^{-\lambda}$ in the case of continuous decay) is multiplied with itself $x$ number of times (keep in mind that $(a^b)^c = a^{bc}$). Multiplying the rate for each new step in time means that the value of $\lambda$ is applied to the current value of $x$ rather than to the initial value. For example, let $\lambda$ be $1/5$ or $0.2$, so that there is a 20% risk of being harvested within a week, and thus 80% chance of being left in the field. The expected lifetime of a strawberry is then $5$ weeks, and the probability of surviving $6$ weeks or more is $P_X(6) = e^{-(1/5)6} \approx 0.301$ or 30,1%, while that of surviving $7$ weeks or more is $P_X(7) = e^{-(1/5)7} \approx 0.247$ or 24,7%, corresponding to a relative decrease in probability of $\frac{(0.301-0.247)\,100}{0.301} \approx 18$ %, equal to $1-e^{-1/5}$. Note that using the rate directly as the base, rather than as exponent of $e$, will give the same results as a (discrete) geometric distribution, in which case the reduction would be exactly the rate between each period. The difference lies in what is termed compound and simple interest in economics. For all the purposes discussed in this thesis, the continuous exponential distribution (with $e^{-\lambda}$ as base to $x$) is deemed more appropriate than the discrete geometric distribution.

From an archaeological point of view, the essential point from the strawberry example above is that this process will also materialise in the size distributions of each single harvest, of all the strawberries at the depot, as well as the strawberries in a finished box for sale, all of which will be exponentially distributed (unless there is some additional sorting process involved). Of course, this does not change the fact that the box weights, as well as the mean strawberry size per box will be normally distributed, as previously shown. When it comes to house sizes, several scenarios involving growth could explain an exponential distribution. Let house size ($x$) be directly dependent on household size, so that each inhabitant has a constant average number of m^2^ of roofed space. Say that the households grow exponentially at some rate, that is they increase in size by a factor of $\lambda_1$ each cycle of some unit length ($y$). At the same rate, the households also extend their houses proportionally to their growth, or replace their house altogether with a bigger house, and lastly, for each new cycle a new household is added to the village with newcomers, so that the total number of households follows $y$ linearly. If all new households start at some minimal size $x_{min}$, the size of a house after $y$ cycles will equal $x_{min}e^{\lambda y}$, and we can solve for its age (time since establishment of the household) as $y = \frac{\log(x)-\log(x_{min})}{\lambda_1}$. Here I use $x_{min}$ for $\theta$ in @johnson1994, p. 494, following @clauset2009, p. 664. In other words, this context would generate an exponential house-size distribution of coeval houses in a village, where the largest houses are those of the households which were the first to settle in the village.

This model is of course not very realistic. For example, households cannot grow without limit, so the larger they become, the higher the probability that they split into two or more factions [see @alberti2014 and @johnson1982, and Section \@ref(social-hierarchy)]. The splitting of households itself can in fact also generate an exponential distribution. Let the households in a village grow linearly -- say they each have a constant surplus of 1 person per year (persons who arrive or are born -- persons who leave or die = 1) -- but they also run a risk of $\lambda_2$ (*e.g.* of 5%) of disintegrating and being replaced by a minimal sized household ($x_{min}$) each year ($y$), no matter their current size. House sizes are then linearly correlated with their age (or the age of the establishment of the household) so that $y = x - x_{min}$ (disregarding the constant of m^2^/inhabitant). However, over time, the probability that a household continues to exist without splitting, will decrease exponentially, and we can write the survival function for households as $P(X > x) = e^{-\lambda_2 y}$, or $e^{-\lambda_2 (x-x_{min})}$.

But again this model is not very satisfactory, particularly since it assumes purely linear population growth, which is unlikely in most cases. It would also seem likely that the probability of splitting of households would not be the same across the range of sizes, but rather be concentrated around some upper threshold, determined by the social structure between the inhabitants or by material or ecological constraints (or a combination). Common for both of the models above, is that one aspect is well described as exponential, but this aspect is only one out of many in the complex process which may underlie the house-size distribution of a settlement. Intuitively, it would also seem strange to have the whole range of houses in a settlement to scale exponentially, since it would imply that single house sizes would be ever closer and closer to each other all the way down to the smallest house in the village. Adding the $x_{min}$ parameter does change this intuition though, as it could then apply to cases with some upper class in which household size and/or wealth would increase to some fixed rate over time, not affecting the size of the main part of the settlement's households, which could well be normally distributed. Or the two models above could be combined to one, pulling exponentially in opposite directions (note that the two rates, $\lambda_1$ and $\lambda_2$ are positive and negative respectively). In both cases, the resulting house-size distribution would no longer be exponential, and these common combination distributions -- the log-normal and the power law, both of which are heavy-tailed -- are presented in more detail below.

Another issue with exponential distributions resulting from growth or decay in time, from the archaeological point of view, is that the time-averaging that so often infiltrates our analyses because of the difficulty of distinguishing temporally coeval data sets, may very well influence the data distribution. This influence is much more challenging to evaluate theoretically, however, so in this thesis it is instead addressed empirically in Section \@ref(distfit-synth).

### Log-normal distributions and the Yule process

fdsasd

### Power-law distributions, hierarchy and scale invariance

fdsa

-   Notes on terminology: Power law, Pareto and Zipf, @newman2005. Lack of consensus on notation, also for scaling parameter. Difference between PDF and CDF, continuous and discrete.

-   A law (distribution) is not a law (of nature), see @grove2011 for review of the long-lasting confusion in archaeology (e.g. @hodder1979), also "rank-size rule"

fsdasdf

### Combinations: Log-normal, stretched exponential, parabolic fractal

## Fitting heavy-tailed distributions in archaeology {#distfit-archaeo}

-   Lit. use @strawinska-zanko2018, @crabtree2017, @maschner2003, @grove2011 ++

-   Zipf law and Settlement Scaling theory, @bettencourt2021, @gomez-lievano2012, @lobo2020. Connection with Central Place Theory, e.g. @müller-scheeßel2007, @chen2011. Why I'm not doing settlement scaling in this study.

-   Not fitting distributions in archaeology, just assuming they are heavy-tailed, or avoiding the question: ex. @brink2013 (could include lots more!)

END chapter
