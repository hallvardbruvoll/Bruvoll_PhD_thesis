# (PART) Size distributions {.unnumbered}

# House sizes and social meaning {#house-sizes-theory}

## Possible reasons for house-size difference

In archaeology there are two recurrent and seemingly contradicting assumptions underlying interpretations of house-size differences within a society. In studies where the goal is to provide population size estimates, this is often calculated from total living area, adding together the areas of the houses in question, and multiplying by a mean value of surface per inhabitant (**give examples**). This mean value is generally obtained from multiple ethnographic parallels. With this assumption -- that every inhabitant requires a similar amount of living space (**mention actual values from literature**) -- the size of a house effectively reflects its number of inhabitants. The second assumption, which is more frequently seen in studies focussing on wealth inequality and social stratification, is that house-size differences are expressions of differences in some sort of wealth or power (**give examples**). In this view, a larger house would have belonged to a wealthier household, capable of procuring more raw materials and activating a larger labour force for its construction and maintenance, in which case there would be significant disparities in living space per individual. Even though there is not necessarily any contradiction between these two interpretations from an anthropological point of view, most archaeologists seem to be unable to consider both possibilities simultaneously. From a methodological point of view, each of these assumptions will tend to mask our ability to see traces in the archaeological data relating to the other assumption -- that is, with existing methods we cannot convincingly provide estimations of both population size and level of social inequality from the same data, even though house sizes frequently form the basis of both argumentations. Far from proposing a solution to this issue, my argument here is that a variety of social institutions known from ethnography and historical sources can explain some level of correlation between the two variables. Dowry and bride price are geographically and temporally widespread practices that link number of offspring (daughters, sons or both, depending on cultural context) with wealth. Clan leaders may draw upon kinship ties and dependencies in order to obtain the workforce needed to construct a larger house (**citation needed**). Nevertheless, if the notion of wealth is at all to be applied meaningfully to non-capitalistic societies, it should designate cases where there is significant and persistent material disparities between members of a population, and not simply point to different household sizes. House sizes could thus potentially reflect a somewhat more complex culture-specific interplay between household size and wealth, meaning that for a given house size, one could assume a range of possible values of the two parameters (**insert fig/illustration**). This relationship between household size, wealth and house size should be studied more in detail empirically through the available ethnographic data, rather than reducing its complexity to a mean surface area per inhabitant for the entire population. Such a study, however, lies beyond the scope of the present thesis. Here I will largely leave aside the question of population and household size, focussing on distribution types of house size data, arguing that the most unequal distribution type considered here -- the power-law distribution -- is unlikely to emerge only from random differences in household size and standard marital patterns, favouring thus interpretations relating to wealth and/or power differences.

Even though the goal here is not to investigate household sizes but to focus on the material aspect of house-size distributions, some fundamental issues of terminology should be addressed. The use of the word *house* (in the wider material sense rather than the Lévi-Straussian sense, see Section \@ref(social-hierarchy)) is indicative of an underlying assumption that the building in question was in use primarily for domestic purposes -- essentially, a fixed architectural unit where someone would spend their nights at least most of their time, and in many cases also cook and eat their main meal during the day -- though proving this directly is not always straightforward in archaeology. For the cultural contexts discussed here -- the Linear Pottery and Trypillia groups in the Neolithic-Chalcolithic -- there is however little evidence for buildings with specific non-domestic (e.g. economic, religious or administrative) purposes, with the probable exception of the so-called mega-structures or assembly houses in the Trypillia mega-sites, which are discussed more in detail below. This lack of evidence does not imply that there was no specialised economic, religious or administrative activity in these societies, as ethnography and history clearly shows that such activity must reach a certain degree of specialisation before it materialises in distinct buildings devoted exclusively for these functions. Artisans, shamans and chiefs could be specialised to some extent but still perform their activity at their domestic home or more diffusely outdoors or without any fixed location [e.g. @costin1991, p. 25]. It is in any case of common usage to speak of houses when discussing architectural units in Neolithic Europe and other prehistoric contexts, maybe because of a lack of a better generic word, but this usage should not prevent archaeologists from recognising other non-domestic functions of buildings whenever there is evidence for it. The *household* is furthermore the designation of all the people, genealogically related or not, usually living under the same roof or within the same architectural unit, constituting a functional whole economically and socially, and potentially including more than a single family unit, as well as servants or slaves, depending on the context.

Kinship studies within anthropology have shown over the last decades that in most societies, contrarily to common misconception, kin affiliation is *not* simply a matter of biological relatedness. In an attempt of grouping together all possible justifications for kinship ties, Marshall Sahlins [@sahlins2013] defined kinship as "mutuality of being" -- that some real or imagined substance is shared, and that this substance is not necessarily genes, as is mostly the case in modern Western societies (with adoption as the main exception). A variety of non-genetic foundations of kinship are widely accepted in different cultures, like sharing of name, time or place of birth or childhood, food source, shared experiences, blood ties, and so on. At the same time, archaeology as a whole has arguably been very slow in taking this diversity of kinship configurations into account, far too often taking for granted the modern Western (especially post-war 20th century) ideal of patrilineal nuclear families as the default configuration for all of human history [e.g. @ensor2021]. That being said, anthropological kinship studies such as that represented by Sahlins typically shows little concern with material culture, and is mostly silent on the question of who -- at the end of the day, quite literally -- sleeps under the same roof, making it hard to interpret domestic architecture in terms of kinship structure and social organisation. The recent comeback of kinship studies in archaeology has been far more focussed on linking isotope and aDNA data with social structures, largely fuelled by the rapid developments of the related methodologies [e.g. @carpenter2022 ; ]

-   Kinship and households: who lives in a house? @sahlins2013, @ensor2013, @ensor2017.. archaeology: @carpenter2022, @madella2013, @joyce2000, @blanton1994, @carsten1995, @hofmann2013, @wilk1982

-   Do clan leaders have bigger houses? check @haude2019, @kahn2021, @bradley2013, @wilk1983.

Persons who possess enough food resources to create obligations through gifts and feasting... What is wealth? House construction as communal activity @goodale2021

P. Květina and J. Řídký point out both architecture (construction, size, orientation) and settlement layout as possible distinctive features between Big Men (achievement-based) and Chief societies, arguing that the former type may be recognised by a dispersed intra-settlement layout combined with uniform architecture, while the latter type would tend towards more regular settlement layout and more marked differences in architecture [! @kvetina2019, p. 13]. EXPAND UPON THIS.

Discuss some archaeo references

-   Schiesberg 2010 2016, go through refs in Zotero, family size and houses for the LBK.
-   Post-marital residency and (average) house size: @hrncir2020.

Functional difference:

-   Ethnography of initiation houses, communal/assembly houses, ritual houses, including @barley2011, @godelier1986, @wilk1983, @fraser1968, @haude2019

-   Caveats: building materials and constraints, climate (heating), mobility, multi-floored, see @porcic2012, residence pattern and floor area in @porcic2010.

## Distribution types and their underlying mechanisms {#distributions}

The main question in this part of the thesis is on the nature of house-size differences in the studied contexts, and to explain them either as being due to random fluctuations or as a material expression of a more structural inequality, or something in-between. In statistical terms, this is a question of distributions. A distribution is a mathematical model of the spread of data along a variable or axis, and it can be modelled on empirical data as a succinct description, or used to predict unobserved data (e.g. future developments, or, in the case of archaeology, data that is lost to taphonomy). In theory, there is an infinite number of possible distributions, as there is no limit to how many parameters one can include to fit the data. It is however generally considered good practice in statistics to limit the number of parameters and identify the simplest possible model that gives a good fit, since a larger number of parameters can often lead to a better fit, but at the same time be harder to explain in terms of underlying mechanisms. Adding many parameters only to achieve a marginally better fit is referred to as *overfitting* (#ref?), and for most real-world contexts there is a limited number of model types that can be considered reasonable candidates. In cases where we are interested in inequality or uneven distributions of data (so-called *skewed* distributions), the most likely distribution models are those that can be described as *heavy-tailed*, meaning that on typical graphical representations (like density/PDF plots or histograms) they will show a characteristic stretch of some of the data towards the right end of the x-axis, while most of the data remains on the left side (Figure \@ref(fig:04-PDF)). The opposite orientation is also possible in theory, in which case the distribution can be referred to as left-tailed. It is important to keep in mind however, that not all distributions are heavy-tailed, and that other distributions also model the spread of data across a variable, but result from very different underlying mechanisms. Identifying the most likely distribution model for a data series is therefore crucial for understanding how the data could have been generated. And though it is true that one model type can have multiple different explanations -- in an archaeological setting for example, many different social behaviours can lead to the same material outcome, an issue known as *equifinality* -- excluding one or more model types for the observed data can help limiting the number of plausible interpretations considerably.

Distributions can be represented graphically in a number of ways, most commonly in a coordinate system with the variable on the x-axis and its density, or *probability density function* (PDF), on the y-axis (Figure \@ref(fig:04-PDF)). Often, and in many of the sources cited in this part of the thesis [e.g. @clauset2009], the PDF is denoted by $p(x)$, reading as *the probability of x*, or simply $f(x)$ (*the function of x*). It gives the probability for a drawn sample ($X$) of falling within a given arbitrarily short range of the distribution, written $Pr(x \le X < x+dx)$. By definition, the area between the PDF curve and the x-axis sums to 1. For reasons that are further discussed in Chapter \@ref(methods-distfit), heavy-tailed distributions, and power laws in particular, are instead often represented with their *cumulative distribution function* (CDF), which is the integral of the PDF (inversely the PDF is the derivative of the CDF; Figure \@ref(fig:04-cCDF)). Similarly to the PDF, the CDF is often denoted as $P(x)$, depending on disciplinary tradition. It indicates the probability of a random sample value being equal to or lower than the function value, or $Pr(X \le x)$. Furthermore, the specific version of the CDF used for plotting heavy-tailed distributions, is the *complementary* or upper tail CDF (sometimes referred to as the *survival function*, or denoted cCDF), which is $1-CDF$ or $P(X \ge x)$, indicating the probability that a random sample is higher than the function value. Both axes on such cCDF plots are traditionally set on logarithmic scales, usually $\log_{10}$ for readability. To avoid confusion, in this thesis I refer to PDF and cCDF for density and distribution functions respectively (except in equations, where I use $p(x)$ and $P(x)$ respectively), and all PDFs are plotted on linear scales and cCDFs on $\log_{10}$ scales, unless otherwise stated. Apart from scales on plots, whenever I refer to logarithms (i.e. in calculations), I imply natural logarithms, that is $\ln$ or $\log_e$ where the base number $e \approx 2.718$.

(ref:04-PDF) Example curves of the probability density function (PDF) of four common distribution types: normal (blue, $\mu = 50$, $\sigma = 8$), exponential (red, $\lambda = 0.1$), log-normal (green, $\mu = 3$, $\sigma = 0.5$) and power-law (purple, $\alpha = 3$, $x_{min} = 1$), in linear (a) and logarithmic scales (b). Parameter values are arbitrary and x-axis is truncated at $2 < x < 100$ for readability. The power-law distribution is the only to form a straight line when both scales are logarithmic

```{r 04-PDF, fig.cap="(ref:04-PDF)"}
load("Results/fig04_PDF.RData")
fig04_PDF
```

(ref:04-cCDF) The same distributions as in Figure \@ref(fig:04-PDF), but with the complementary (right-tail) cumulative distribution function (cCDF), in linear (a) and logarithmic scales (b), with $0 < x < 80$ for readability

```{r 04-cCDF, fig.cap="(ref:04-cCDF)"}
load("Results/fig04_cCDF.RData")
fig04_cCDF
```

In the following, I present briefly the main distribution types that will be discussed further in the following chapters, with special focus on the *power-law distribution*, which is the model type associated with fractals and structural hierarchy. All of these distributions are modelled on continuous univariate data series [see @johnson1994 for more detailed presentations]. Even though their mathematical definitions may seem complicated to non-initiated readers, most of the distribution types discussed in this thesis are readily implemented in standard statistical software, including Microsoft Excel, allowing for a more straight-forward use of them. For this thesis, I used base *R* functions for calculating PDFs and cCDFs, and for random number generation for normal, log-normal and exponential distributions [@R2023], and equivalent functions from the *poweRlaw* package for power-law and Weibull/stretched exponential distributions [@gillespie2015].

\FloatBarrier

### Normal distributions and the Central Limit Theorem

One of the distribution models that are the most commonly referred to and well-known, is the *normal distribution*, also known as the "Bell Curve" due to the characteristic bell shape of its PDF (Figure \@ref(fig:04-PDF)a), or Gaussian after mathematician C.F. Gauss who contributed to its exploration in the early 19^th^ century. Its characterising parameters are the *mean* (denoted $\mu$, *mu*) and *standard deviation* ($\sigma$, *sigma*), and the PDF is defined mathematically as

```{=tex}
\begin{equation}
p(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-(x-\mu)^2/2\sigma^2}
(\#eq:normal)
\end{equation}
```
where $e$ (*Euler's number*) and $\pi$ (*pi*) are mathematical constants [@johnson1994, pp. 80-8]. This distribution is centred around its mean, with dwindling amounts of data spread outwards to either side. The mean -- commonly known and widely used in daily speech -- is the sum of all observations divided by number of observations, or

```{=tex}
\begin{equation}
\mu = (\sum_{i=1}^nx_i)\,\frac{1}{n}.
(\#eq:mu)
\end{equation}
```
The spread of the data from the mean is defined by the standard deviation, which is the square root of the mean of all squared deviations from the overall mean, or

```{=tex}
\begin{equation}
\sigma = \sqrt{(\sum_{i=1}^n(x_i-\mu)^2)\,\frac{1}{n}}.
(\#eq:sigma)
\end{equation}
```
Mathematically, the square of the standard deviation, $\sigma^2$, or *variance*, is simpler, but since it is also less intuitive, I refer here to the former whenever possible. The standard deviation can be thought of as the mean of all deviations, positive or negative, from the overall mean.

The great importance the normal distribution has to a wide range of phenomena is explained through the *Central Limit Theorem* (CLT), according to which the sum of random variables tends to a normal distribution as the number of variables increases towards infinity, under certain conditions [@johnson1994, pp. 85-8]. More specifically, if $X_1, X_2, \dots, X_n$ are independently drawn and identically distributed (condition referred to as *i.i.d*) random variables or samples, their sum will be normally distributed in the limit as $n$ tends to infinity, or

```{=tex}
\begin{equation}
\lim\limits_{n \to \infty} \sum_{i=1}^n X_i = \mathcal{N}.
(\#eq:CLT)
\end{equation}
```
The sum may be standardized in some way to avoid infinite numbers, but in practice $n$ will always be limited, so the resulting distribution will always also be approximately normal at best. The original distribution of the random variables (i.e. that of the *population*) does not need to be normal for the theorem to hold, but can be any distribution as long as its variance is finite. The normal distribution (referred to as the *sample distribution* in this context) is related to the population distribution of the random variables (samples), in that the mean of the population equals the mean sum of the sample distribution divided by $n$, and the standard deviation of the sample distribution, referred to as the standard error $s$, equals the standard deviation of the population divided by the square root of $n$, or $s = \frac{\sigma}{\sqrt{n}}$, meaning that $s$ is reduced with higher $n$ (i.e. larger sample size). A common variant of the CLT is that the distribution of sample means, rather than sums, are normally distributed, in which case the mean of the sample distribution equals the mean of the population distribution.

In more laypersons' terms, the total weight of a box of strawberries is the sum of the weights of the strawberries it contains (subtract the weight of the box itself, and assume the same number of berries per box for the sake of argument). In the packaging facility, a large population of strawberries are continuously distributed randomly into boxes of the same size. The berries in the factory (the population) have weights that follow some distribution with limited variance -- some are bigger than others, but there are upper and lower limits to how much a strawberry can weigh -- and all the berries in the boxes are drawn from this same population (so identically distributed). New berries are shipped to the facility all the time, and berries are not sorted according to size but randomly mixed, so if one box by chance gets filled with only very large berries, that does not affect the weight of subsequent boxes (each box is the sum of $n$ independently drawn sample berries $X$). Under these circumstances, the weight of boxes of $n$ berries (i.e. the sum of the berries' weights, the sample distribution) will be normally distributed by the Central Limit Theorem.

Some details here are crucial: the distribution of the strawberry weights themselves will, with higher $n$, tend towards that of the population, which is not necessarily normal. The theorem is only valid for summary measures like sums or means, and not for the observations directly. If the strawberries at the facility are mostly large (heavy) and with only smaller proportions of small berries, this skewed distribution will be reflected in strawberry boxes of a certain size ($n$), but since the influence of this distribution is the same on all boxes of the same size, their overall weights (or mean weights) will be normally distributed. Furthermore, if we draw some boxes from one producer and some from another producer who has significantly larger or smaller berries, the samples are then drawn from different populations and are thus not identically distributed, and the sample distribution will not necessarily be normal (colloquially referred to as "comparing apples and oranges"). Similarly, if there is an overall trend of boxes becoming heavier over the season, then samples from across the season will not be normally distributed. If however, we compare the mean box weights from multiple seasons, these will again be normally distributed, unless there is also a multi-year trend of strawberries becoming larger or smaller. In other words, the i.i.d. condition of the CLT means that the samples are drawn at random, with no important underlying trends. As a side note, there is a substantial body of research on the conditions under which normal distributions can emerge *without* the i.i.d. condition being met [see @johnson1994, pp. 87-8 for details and further references].

Also important to note is that the number of samples $X_n$ (boxes of sample size $n$ in the example above) does not matter to the shape of the sample distribution, other than to the resolution of the curve or binwidth of the histogram when plotting, and the weight of a single sample can be modelled as a probability following a distribution. The normal curve of the sample distribution can be entirely defined by the population $\mu$ and $\sigma$, and sample size $n$. In many practical settings however, the parameter values of the population distribution (and even its type of distribution) are unknown, in which case more samples are needed in order to model a sample distribution and from there infer the parameter values of the overall population. This is the case when statistical tests like Student's t-test or ANOVA are applied for examining the relations between samples and populations.

In other cases, $n$ (sample size) can be unknown, but assumed to be approximately the same for all $X_n$ (samples), like in the somewhat more abstract cases where each of the $n$ variables contained in $X_n$ are of different nature -- or stated otherwise, when the size of $X_n$ is the sum of many different and independent causes. In the case of a normal house-size distribution within a given cultural setting (like a village), one can assume that the same number of causes affect the size each house takes when constructed, but to varying degrees (colloquially we often say *factors* for such causes, though in this setting one should strictly speaking prefer *terms*, *summands* or *addends*, since they add up rather than multiply). Say that house size in a given context results from the cumulative effects of household size, inherited wealth, soil quality, exposure to sun, wind or flooding, artisan specialisation, raw material availability in the year of construction and many more variables. Each of these may have separate probability distributions -- e.g. the wealth distribution may be heavy-tailed while household size may be normal and symmetric -- but as long as the overall population, so to speak, of contributing causes is distributed in the same way to all households, and that none of the variables dominates the effect of the others, and the value of each variable is independent from the values of the other variables, their sums expressed in house sizes should be normally distributed by the CLT. Such a distribution is then an expression of *random difference*, and in the case of house sizes, there would be no particular reason as to why a few houses would be bigger than all the others, just as a few houses would also be smaller, while most would be centred around the mean size. But again, if samples are drawn from *different* populations, e.g. houses from different villages or cultural contexts -- where the probability distributions of the underlying variables are categorically different -- the i.i.d. condition is not met, and the distribution of house sizes is unlikely to be normal. Likewise if the population contains grades, i.e. is grouped, or where the different variables are correlated between them, e.g. if wealthier households are also larger households and have more access to raw material and better quality soils, and so on, their house sizes may also become disproportionately large and deviate from normal expectations.

A final caveat for normal house-size distributions that may be mistaken for skewed ones, is the case when there are no significant differences between house sizes, except for one that has a clearly different function -- as in the ethnographic cases of community houses and men's houses discussed above. Then it makes little sense to interpret the resulting slight skewness of the distribution as a sign of social inequality in itself (notwithstanding gender inequalities). As a rule of thumb, to avoid such misinterpretations, it is useful to test whether isolating the single largest house changes the retained model when performing distribution fitting.

### Exponential distributions and constant rates of growth and decay

The exponential distribution is -- next to the normal -- one of the most widely applicable distribution models. In its simplest form, it is a function of a positive random variable $x$ where some base number (usually $e \approx 2.718$, for compound continuous growth) is raised to the power of $-x$, in other words when $x$ has the probability density $$p(x) = e^{-x}.$$

This simple form is called the *standard* exponential distribution, and in most practical applications there will also be a *rate* parameter $\lambda$ (*lambda*), so that the density function becomes

```{=tex}
\begin{equation} p(x) = \lambda e^{-\lambda x}.
(\#eq:exponential)
\end{equation}
```
In the case of the standard version, $\lambda = 1$ and can be left out. The negative rate in the exponent is the actual rate that determines the shape of the distribution, whereas the rate multiplier to the base is a *normalising constant* which assures that the area under the curve adds up to 1, and thus that the values shown on the y-axis are probabilities. This constant can be though of as the y-intercept of a linear model, since at $x = 0$ the function gives $\lambda e^0 = \lambda\,1$. This is seen more clearly if we take the logarithm of the exponential density function, $p(log(x)) = log(e)\,(-\lambda x) + log(\lambda) = -\lambda x+log(\lambda)$, which is a linear model with slope $-\lambda$ and $log(\lambda)$ as y-intercept. As a rule of thumb, an exponential distribution can thus also be recognised as a straight line on a plot with one linear and one logarithmic axis. A wide variety of more complex forms have been formulated, and the distribution type is furthermore generalisable to both the Gamma and Weibull distributions [@johnson1994, pp. 494-9]. Note that the simple form presented here may also have more complex notations in specialised statistical literature, *cf*. Eq. 19.1 in @johnson1994, which is equivalent to the density function above given that $\sigma^{-1} = \lambda$ and $\theta = 0$ [see also @johnson1994, pp. 522-3; @clauset2009, p. 664].

Exponential distributions have the highest probability (so the most data) at low values of $x$, and ever lower probabilities towards the right end of the curve (Figures \@ref(fig:04-PDF)a and @ref(fig:04-cCDF)a), with the rate of decrease determined by $\lambda$ -- the higher the rate value the steeper the curve falls off from left to right, and inversely low $\lambda$ values give more heavy-tailed distributions. The type of setting which is most commonly modelled as an exponential distribution, is that of "events recurring at random in time" [@johnson1994, p.494]. If $x$ represents the the duration in time between events (or duration of single events) that occur continuously and independently from each other, with a constant average rate of $\lambda$ events per unit of time, it can be modelled as an exponential distribution. An important feature of the underlying process (a so-called Poisson point process), is that it is memoryless, meaning that the duration between events $X_1$ and $X_2$ does in no way affect the duration between $X_2$ and $X_3$, as all durations are drawn independently from the distribution with the same average rate $\lambda$. The example of such a process that may be the most familiar to archaeologists, is the radioactive decay of the ^14^C isotope with its average decay rate $\lambda \approx 0.00012$ or 0.012% per year. With a rate that low and using years as the time unit, it is more useful and intuitive to work with the *half-life* measure, or the time it will take on average for the initial quantity to be halved, which for ^14^C is about 5730 years. The half-life (the median of the distribution) is given by $\log (2)/\lambda$, solving for $x$ in the cCDF function $e^{-\lambda x} = 1/2$, where the normalising constant in the PDF is replaced with 1 (and therefore omitted in multiplication) for the initial quantity or y-intercept. If we replace $1/2$ with the remaining proportion of ^14^C in the organic material of an ancient artefact, compared to the expected amount in the same material when alive, we can use the same equation to solve for the approximate year when the organic material died, which is the principle behind radiocarbon dating. In a radioactive decay process, a total amount of individual unstable isotopes are present from the start (here the death of an organic material), and their individual lifetimes until decay are exponentially distributed. As an additional metric of exponential random variables, the mean $\mu$ or *expected value* is given as the reciprocal of $\lambda$, so that $\mu = 1/\lambda$ and $\lambda = 1/\mu$. The mean is larger than the half-life, and for ^14^C decay, this corresponds to $\mu \approx 1/0.00012 \approx 8267$ years.

The exponential distribution can also model many processes that are closer to an everyday human scale than ^14^C decay. Expanding from the example used for normal distributions, let $\lambda$ be the average risk for a strawberry of being harvested within a week $x$. As the weeks go by (as $x$ increases), the cumulative risk of being picked grows exponentially, so that there are very few berries that are older than a few weeks, and the mean age of the berries in the field is $1/\lambda$. The berries are furthermore picked by a harvesting machine that is unable to aim for a certain size category, and the berries grow linearly (which may be a rather poor approximation, but for the sake of argument). The harvested berries are also continuously replaced by new berries which start growing at the same pace, so the field is always renewed. Under these circumstances, the lifetime of a strawberry during which it grows is exponentially distributed, and the probability of surviving $x$ weeks follows Equation \@ref(eq:exponential)). The example illustrates how exponential functions model repeated multiplication or multiplicative processes, since the $x$ in the exponent means "multiplied $x$ number of times". The rate $\lambda$ (technically $e^{-\lambda}$ in the case of continuous decay) is multiplied with itself $x$ number of times (keep in mind that $(a^b)^c = a^{bc}$). Multiplying the rate for each new step in time means that the value of $\lambda$ is applied to the current value of $x$ rather than to the initial value. For example, let $\lambda$ be $1/5$ or $0.2$, so that there is a 20% risk of being harvested within a week, and thus 80% chance of being left in the field. The expected lifetime of a strawberry is then $5$ weeks, and the probability of surviving $6$ weeks or more is $P_X(6) = e^{-(1/5)6} \approx 0.301$ or 30,1%, while that of surviving $7$ weeks or more is $P_X(7) = e^{-(1/5)7} \approx 0.247$ or 24,7%, corresponding to a relative decrease in probability of $\frac{(0.301-0.247)\,100}{0.301} \approx 18$ %, equal to $1-e^{-1/5}$. Note that using the rate directly as the base, rather than as exponent of $e$, will give the same results as a (discrete) geometric distribution, in which case the reduction would be exactly the rate between each period. The difference lies in what is termed compound and simple interest in economics. For all the purposes discussed in this thesis, the continuous exponential distribution (with $e^{-\lambda}$ as base to $x$) is deemed more appropriate than the discrete geometric distribution.

From an archaeological point of view, the essential point from the strawberry example above is that this process will also materialise in the size distributions of each single harvest, of all the strawberries at the depot, as well as the strawberries in a finished box for sale, all of which will be exponentially distributed (unless there is some additional sorting process involved). Of course, this does not change the fact that the box weights, as well as the mean strawberry size per box will be normally distributed, as previously shown. When it comes to house sizes, several scenarios involving growth could explain an exponential distribution. Let house size ($x$) be directly dependent on household size, so that each inhabitant has a constant average number of m^2^ of roofed space. Say that the households grow exponentially at some rate, that is they increase in size by a factor of $\lambda_1$ each cycle of some unit length ($y$). At the same rate, the households also extend their houses proportionally to their growth, or replace their house altogether with a bigger house, and lastly, for each new cycle a new household is added to the village with newcomers, so that the total number of households follows $y$ linearly. If all new households start at some minimal size $x_{min}$, the size $x$ of a house after $y$ cycles will equal $x_{min}e^{\lambda_1 y}$, and we can solve for its age (time since establishment of the household) as $y = \frac{\log(x)-\log(x_{min})}{\lambda_1}$. Here I use $x_{min}$ for $\theta$ in @johnson1994, p. 494, following @clauset2009, p. 664. In other words, this context would generate an exponential house-size distribution of coeval houses in a village, where the largest houses are those of the households which were the first to settle in the village.

This model is of course not very realistic. For example, households cannot grow without limit, so the larger they become, the higher the probability that they split into two or more factions (see Alberti 2014 and Johnson 1982, and Section \@ref(social-hierarchy)). The splitting of households itself can in fact also generate an exponential distribution. Let the households in a village grow linearly -- say they each have a constant surplus of 1 person per year (persons who arrive or are born - persons who leave or die = 1) -- but they also run a risk of $\lambda_2$ (e.g. of 5%) of disintegrating and being replaced by a minimal sized household ($x_{min}$) each year ($y$), no matter their current size. House sizes are then linearly correlated with their age (or time since establishment of the household) so that $y = x - x_{min}$ (disregarding the constant of m^2^/inhabitant). However, over time, the probability that a household continues to exist without splitting, will decrease exponentially, and we can write the survival function for households (and therefore their sizes) as $P(X > x) = e^{-\lambda_2 y}$, or $e^{-\lambda_2 (x-x_{min})}$.

But again this model is not very satisfactory, particularly since it assumes purely linear population growth, which is unlikely in most cases. It would also seem likely that the probability of splitting of households would not be the same across the range of sizes, but rather be concentrated around some upper threshold, determined by the social structure between the inhabitants or by material or ecological constraints (or a combination). Common for both of the models above, is that one aspect is well described as exponential, but this aspect is only one out of many in the complex process which may underlie the house-size distribution of a settlement. Intuitively, it would also seem strange to have the whole range of houses in a settlement to scale exponentially, since it would imply that single house sizes would be ever closer and closer to each other all the way down to the smallest house in the village. Adding the $x_{min}$ parameter does change this situation though, as it could then apply to cases within some upper class in which household size and/or wealth would increase to some fixed rate over time, not affecting the size of the main part of the settlement's households, which could well be normally distributed. Or the two models above could be combined to one, pulling exponentially in opposite directions (note that the two rates, $\lambda_1$ and $\lambda_2$ are positive and negative respectively). In both cases, the resulting house-size distribution would no longer be exponential, and these common combination distributions -- the log-normal and the power law, both of which are heavy-tailed -- are presented in more detail below.

Another issue with exponential distributions resulting from growth or decay in time, from the archaeological point of view, is that the time-averaging that so often infiltrates our analyses because of the difficulty of distinguishing temporally coeval data sets, may very well influence the observed data distribution. This influence is much more challenging to evaluate theoretically however, so in this thesis it is instead addressed empirically through simulation in Section \@ref(distfit-synth).

### Log-normal distributions and Gibrat's law

One of the main candidate models for heavy-tailed continuous distributions is the log-normal, defined as a variable $x$ of which the logarithm is normally distributed. Adapted from Equation \@ref(eq:normal), its density function can be written [following the notation in @mitzenmacher2004, p. 229] as

```{=tex}
\begin{equation}
p(x)=\frac{1}{\sqrt{2\pi}\sigma x}e^{-(\log(x)-\mu)^2/2\sigma^2}.
(\#eq:log-normal)
\end{equation}
```
The $\mu$ (mean) and $\sigma$ (standard deviation) parameters are usually understood as the equivalent values associated with the normal distribution of $\log(x)$. It can be thought of as a combination of the normal and the exponential distributions, and like these, it has been shown to apply well to a wide range of natural and social phenomena, from the growth of organisms in biology to the pricing of options in finance [see @mitzenmacher2004, pp. 235-7 and @johnson1994, pp. 209-11, 238-40]. One implication of $\log (x)$ being normally distributed, is that the density curve of $x$ will appear as a normal bell curve when plotted with a logarithmic scale on the x-axis (Figure \@ref(fig:04-lnorm-exp)). With linear scales, the curve is skewed with the mode to the left and a tail of high values to the right. More technically, the (natural) logarithm of a variable ($x$) is the variable of exponents that may raise $e$ to the values of $x$. A linear increment in a variable of exponents -- say from 1 to 2 -- will, with the same base, correspond to an exponential increment in powers (the result of exponentiation), as $e^1 \approx 2.718$ and $e^2 \approx 7.389$. Thus, if the exponents of $e$ that correspond to the values of $x$ are normally distributed, then $x$ itself will resemble an exponentially stretched normal distribution, which is a log-normal distribution.

Since exponents represent repeated multiplication and normal distributions result from random additive processes (see above), log-normal distributions may be most easily understood as resulting from random multiplicative processes. The product rule of logarithms states that the logarithm of a product of numbers equals the sum of the logarithms of those same numbers. This can be expressed as $\log (ab) = \log (a) + \log (b)$. Then, since the sums of many random numbers are normally distributed according to the Central Limit Theorem, the logarithm of the product of many random numbers (that is, these numbers multiplied together) should also be normally distributed [see e.g. @newman2005, pp. 347-8 for more elaboration].

The product of many random numbers is typically the result of a growth or decay process where the rate fluctuates randomly. A convenient example, following @newman2005 [p. 348], is that of a financial investment. If an initial value ($a$) is invested in stocks that generate a return ($e^\lambda$) which fluctuates randomly from year to year with a finite variance, the return $y$ after $x$ years will follow a wiggly exponential curve, or $y = a e^{\lambda x}$ (Figure \@ref(fig:04-multi-exp)). After some years, the value of y will follow a log-normal probability distribution (Figure \@ref(fig:04-lnorm-exp)). If several persons start investing the same amount in stocks at the same time, then after a period, say of 10 years, most of them will have earned returns of comparable size, centred around some mean return, while the earnings of the top investor may be several orders of magnitude higher, simply by chance. This assumes however that everyone invests randomly in the stock market, which is rarely the case. More scrupulous investment strategies may reduce the effects of chance and thus the spread of final returns, but this effect may again be countered by the risk-willingness of investors. In either case the resulting distribution after a given time period will be log-normal, which explains why this model is a central tool in financial analyses [e.g. see @mitzenmacher2004, p. 236 for its use in the Black-Scholes option pricing model].

(ref:04-multi-exp) Exponential distribution of $y = \lambda ^x$ with rate ($\lambda$) fluctuating randomly and uniformly between 0.75 and 1.4, i.e. with a mean rate of approx. 1.08, over 40 periods ($x$) from an initial value of 1. The plots figure 100 individual runs of the distribution, with linear (a) and logarithmic (b) y-axis. Over time, y-values at any given x are expected to be log-normally distributed by the Central Limit Theorem

```{r 04-multi-exp, fig.cap="(ref:04-multi-exp)"}
load("Results/fig04_multi_exp.RData")
fig04_multi_exp
```

(ref:04-lnorm-exp) Density function of y-values from Figure \@ref(fig:04-multi-exp) at $x = 40$, with linear (a) and logarithmic (b) x-axis, following a typical log-normal distribution

```{r 04-lnorm-exp, fig.cap="(ref:04-lnorm-exp)"}
load("Results/fig04_lnorm_exp.RData")
fig04_lnorm_exp
```

This process is known as *Gibrat's law*, after the French engineer Robert Gibrat who was the first to demonstrate its wide applicability, offering a mathematical explanation of the skewed size distributions that had frequently been observed by economists [@gibrat1930]. Gibrat argued that the log-normal distribution was better fit for modelling firm sizes and salaries than the one already proposed by Pareto (i.e. the power law, see below), since it could account for the entire size range and not only the tail, and since it was theoretically better founded, as Pareto's original model was purely empirical. He termed the model the *law of proportionate effect*, since it emerges -- as shown above -- when proportionate growth rates are independent of absolute size. This essentially means that growth is exponential (size at any given time is multiplied with a rate, so proportional) rather than linear (additive), and that the range of possible rates is not determined by absolute size. Note however that there must be some randomness in the rate for the growth to result in a log-normal distribution. If the rate is *exactly* the same for all samples, or if it follows the exact same sequence of random rates, the initial distribution will remain the same over time, even though any initial spread will be scaled up or down according to the rates (Figure \@ref(fig:04-still-normal)). In most economic settings, though there are overall trends that may affect everyone in a population (of firms, employees, cities etc.) at large scales, there are also many smaller factors that will affect individuals differently, causing random variation.

(ref:04-still-normal) a: Eponential growth of 100 samples drawn from a normally distributed initial population ($\mu = 10$, $\sigma = 2$), all following the same sequence of uniformly distributed rates ($0.75 < \lambda < 1.4$). b: Over time, $\mu$ and $\sigma$ values change, but the distribution remains normal. Scales are linear

```{r 04-still-normal, fig.cap="(ref:04-still-normal)"}
load("Results/fig04_still_normal.RData")
fig04_still_normal
```

The modern financial market is of course not directly applicable to the Neolithic, but a number of multiplicative processes involving random fluctuations may also be relevant to Neolithic social structure or economy. One obvious example would be that of crop yields over time. Assuming that crop cultivation in a village is organised at a household level, and that households grow their crops at separate locations in the vicinity, random differences in soil quality, sunlight, water, exposure to disease and so on, would arguably generate normally distributed yields in one year (the yield volume being the sum of many random effects). But given that the yield the year after also depends on the current yield through the size of the surplus that will be available for sowing, a randomly large yield one year will have better chances of producing an even larger yield the next year, and so on, in the same way a large financial return of a lucky investor one year is more likely to reach an even higher return later if reinvested. Supposing again that house size reflects household size linearly, and that larger yields can sustain larger households, this process could explain the emergence of a log-normal house-size distribution in a village over time.

For Neolithic crop cultivation it is in many cases more reasonable to assume that cultivation took place very close to or even within the village, in which case the conditions for yield volume would be very similar between households (#ref needed?). Cultivation could also be organised collectively rather than at the single household level, or as a combination depending on the crop. In such cases, a good or a bad harvest would affect all households equally, and skewed distributions would not emerge easily (#Ref. to Kohler 2018). However, even in such scenarios, small random differences in the initial sowing volumes of individual households could grow exponentially over time and produce a log-normal distribution of household yields, though with smaller spread between the highest and lowest values.

A somewhat different mechanism for explaining skewed house-size distributions within Linear Pottery settlements in particular, was proposed by Sara @schiesberg2010. Assuming a stable population over time, with number of children per woman surviving to reproductive age being Poisson distributed with $\lambda = 2$ (with ever decreasing probability of larger numbers of surviving siblings), and the probabilities of having given ratios of male and female children following a binomial distribution, the combined probability of having *x* male children surviving to reproductive age would follow a skewed log-normal-like (though discrete) distribution. Schiesberg observed this theoretical distribution of male siblings to be analogous to the empirical house-size distribution of excavated Linear Pottery settlements on the Aldenhoven plateau in North Rhine-Westphalia, Germany, arguing that there were gaps in the continuous size distribution that fitted with the discrete limits between numbers of male siblings. The correlation was furthermore explained as a result of a mainly patrilocal residence pattern, where house size would be a function of number of sons in an extended family (patrilocality being the most widely accepted residence pattern for the Linear Pottery culture, see Section \@ref(lbk)). The same pattern could have emerged as a function of number of daughters in the case of matrilocality. Though Schiesberg did not model the observed house-size distribution explicitly as a log-normal, her model and the underlying process closely resemble the Gibrat's law described above. A (discrete) Poisson distribution with low rate can well approximate a (continuous) exponential, and a (discrete) binomial can approximate a (continuous) normal -- so a combination of the two (by multiplication of exponential and normal probabilities) will equally approximate a log-normal. Thus, simply by the social practice of patrilocal post-marital residence, a skewed house-size distribution would emerge spontaneously, without the presence of any additional structural inequality between community members. In this case, a log-normal distribution would be present in a settlement from its onset, and the skewness could or could not become more pronounced over time, depending on other factors like whether crop surplus would be distributed within the community or kept within households.

A last mechanism that is sometimes referred to in statistical literature as causing log-normal distributions, is that of random additive processes involving variables that by their nature cannot take on negative values, such as weights, heights or densities of physical entities [e.g. @johnson1994, p. 239]. While (two-parameter) log-normal distributions only can have positive values [@johnson1994, p. 208], normal distributions will often also have positive probabilities below zero, depending on $\mu$ and $\sigma$ values, and are then poor models of such quantities. A house, as an example, cannot have negative size, but will always be larger than some lower threshold above zero, and should thus also be more adequately modelled as log-normal than normal, even if the distribution looks symmetrical. This will in turn allow for more accurate estimates of other derived parameters, like the confidence limits for the coefficient of variation. However, in cases of symmetrical distributions where $\mu$ is much higher than $\sigma$, there is little practical reason to prefer a log-normal model over a normal one, as the probabilities of values below zero will be infinitesimally low.

\FloatBarrier

### Power-law distributions, preferential attachment and hierarchy

A variable $x$ is power-law distributed when its probability follows the power of itself with a fixed exponent $\alpha$, so that $p(x) \propto Cx^{-\alpha}$ [@clauset2009, p. 662]. Such distributions decrease very quickly as $x$ increases, and are thus highly skewed, but the probability never reaches 0 -- it is said to be *asymptotic* -- meaning that they are also very heavy-tailed (Figures \@ref(fig:04-PDF)a and \@ref(fig:04-cCDF)a). Furthermore, a power law can only take positive values, and there is always a minimal threshold $x_{min} > 0$ above which the function holds. The exponent $\alpha$, often termed *scaling exponent* or *scaling parameter*, will usually lie in the range $0 < \alpha < 3$, though values below 1 are considered rare special cases, when considering size or frequency distributions [@newman2005, pp. 331-2]. Low exponent values give more heavy-tailed distributions and *vice versa*, so power laws with a high scaling exponent (around 3 or above) are those that in practice will be more easily mistaken for other less skewed distributions like exponential or log-normal distributions (Figure \@ref(fig:04-pl). As with the previously discussed distribution models, the power law is also usually associated with a normalising constant (here denoted $C$), a factor that ensures that the area under the curve of the PDF sums to 1, and which here is defined as $(\alpha-1)x_{min}^{\alpha-1}$ [@clauset2009, pp. 664-5]. Applying the product rule of exponents, the power-law PDF or density function can be expressed as

```{=tex}
\begin{equation}
p(x) = \frac{\alpha-1}{x_{min}}(\frac{x}{x_{min}})^{-\alpha}.
(\#eq:power-law)
\end{equation}
```
In the cCDF or survival function, again the normalising constant is replaced with 1, but rather than left out it is written in the exponential form $(\frac{x}{x_{min}})^1$ which equals 1 when $x = x_{min}$, so that

``` tex
\begin{equation}
P(x) = (\frac{x}{x_{min}})^{-\alpha+1}
(\#eq:pareto)
\end{equation}
```

in the notation of @clauset2009, Eq. 2.6, equivalent to the more complex notation in @newman2005, Eq. 4.

As shown in Figures \@ref(fig:04-PDF)b and \@ref(fig:04-cCDF)b above, power-law PDFs and cCDFs hold the special property of appearing linear when plotted with logarithmic x and y axes (or equivalently when x and y values are log-transformed). This can be shown by log-transforming the simple functional form above, so that $log(p(x))=(-\alpha)log(x)+log(C)$, which is a linear model with y-intercept $log(C)$ and slope $-\alpha$. Power functions are in this way similar to exponential functions, with the difference that the variable $x$ is here in the base rather than in the exponent, causing the graph to be linear only when *both* axes are logarithmic (as opposed to one axis for exponential functions). Because of this property, the most common method for estimating $\alpha$ since the first formulation of the model and until recently [@clauset2009; @stumpf2012] was to plot the data on logarithmic axes, perform a least squares linear regression and estimate the slope (see Chapter \@ref(methods-distfit)).

The main reason for the great attention that has been given to power-law distributions over the years (see below), is its property of *scale invariance*, meaning that the distribution will appear the same no matter the scale in which it is being observed [see chapters 1 and 2 of @west2017, pp. 1-78 for a general non-technical introduction to scaling in science]. Figure \@ref(fig:04-pl) shows the relation between change in the variable being modelled ($x$) and its function value (here the PDF function) as a constant proportion of exponents (i.e. a linear relation between two vectors of logarithms). This means that for a power-law distributed variable, a change of scale (zooming in or out on the x-axis, changing order of magnitude no matter the base number) will always result in a proportional change of order of magnitude of the probability or frequency modelled on the y-axis. If $x$ represents a size related variable, and $y$ a frequency (probability in the PDF, rank in the CDF, or absolute frequency), this proportional relation in logarithms generates a self-similar hierarchy, where the same structure of sizes and frequencies is repeated across different scales (Figure \@ref(fig:04-hierarchy)). Thus, even though fractal structures are most often associated with geometric shapes in physical space -- like the shapes of plants, rivers or clouds -- they can be equally present in non-geometric variables, like income or wealth, magnitude of events, sizes of cities or nodes in networks. Such structures are, as with physical fractals, recognised by their abrupt skewness or high level of inequality, by their hierarchically distributive mode of functioning, and their statistical signature which is the power law.

(ref:04-pl) Examples of power-law distributions with different scaling exponent ($\alpha$) values on logarithmic axes, showing how this parameter reflects change over orders of magnitude. For a model with $\alpha = 3$ (blue), a decrease in probability p(x) of 3 orders of magnitude (powers of 10), e.g. from 0.1 to 0.0001 corresponds to an increase in the size of x of 1 order. For a model with $\alpha = 1.5$, the same decrease in probability corresponds to an increase in x of 2 orders of magnitude. The models appear linear in logarithmic space, but are in reality highly non-linear, as illustrated by the grid

```{r 04-pl, fig.cap="(ref:04-pl)"}
load("Results/fig04_pl.RData")
fig04_pl
```

(ref:04-hierarchy) A power-law distribution of sizes arranged in discrete levels, illustrating its characteristic scale invariance. From the largest element on top and downwards, sizes decrease while numbers (frequencies) increase, both exponentially but in opposite direction, generating a hierarchical fractal structure where the same shapes are recognised at different scales

```{r 04-hierarchy, fig.cap="(ref:04-hierarchy)"}
load("Results/fig04_hierarchy.RData")
fig04_hierarchy
```

As already mentioned, the power-law distribution was first formulated by Italian economist Vilfredo Pareto as a model of wealth inequality [-@pareto1896]. Over the 20^th^ century it saw an increasing number of applications in a wide variety of fields in natural and social sciences, modelling phenomena from magnitudes of earthquakes and sizes of moon craters to the intensity of wars, frequency of family names and citations of scientific papers [see @mitzenmacher2004; @newman2005 for detailed overviews]. The idea that the power law could be a suitable model for city sizes was seemingly first proposed in a short paper by @auerbach1913, though it is often attributed to @zipf1949, through which it has become known to and sometimes applied by archaeologists (see Section \@ref(distfit-archaeo)). The first influential attempt at an explanatory model of power-law distributions -- which at first were primarily descriptive, one of the main critiques against Pareto -- was made by Udny @yule1925, who seeked to explain the observed size distribution of genera by number of species. Yule's model [here following the notation in @newman2005, pp. 340-2] involves a set of $n$ genera consisting of a variable $k$ number of species each. During a discrete time step interval, a constant $m < n$ number of new species are added to the existing genera by speciation, so that some but not all genera will grow to $k+1$ in each time step. The probability for each genus of receiving a new species in a given time step is proportional to $k$ or the number of species it already includes, since a speciation is more likely to happen in an already large group of species than in a small group. Finally, for each time step one new species is sufficiently different to be considered a new genus on its own, so that $n$ increases linearly by 1 for each step. Under these conditions, over time $k$ will follow a power-law distribution -- or strictly speaking a discrete version of it known as the Yule distribution [see @simon1955].

The precise mathematics involved in this model are quite complex for non-specialists, especially in the version presented by Yule before modern stochastic theory was developed, but even more recent and concise formulations will involve some level of calculus [@newman2005; @mitzenmacher2004, pp. 230-3]. The essential is however to note that the mechanism it describes -- now known as *the Yule process* -- is relatively simple and transferable to many natural as well as social settings. The heavy-tailed distribution of species will emerge even in the simplest scenario starting with one genus $n_1$ consisting of a single species ($k = 1$), and one added species to existing genera per time step ($m = 1$). Then the single genus will have probability $p(n_1) = 1$ of receiving the new species $m$ so that it gets $k = 2$, while the new genus $n_2$ starts at $k = 1$. The next step a new species will be given to one of these two genera, but $n_1$ has twice as high probability of receiving it as $n_2$ -- that is, probabilities are $2/3$ and $1/3$ respectively. In other words, the probability for any genus $n_{i}$ with $k_i$ species of winning the round (so to speak) and receiving the extra species, is given by $k_i/\sum{k}$, or the fraction of the total amount of species (in that time step) that the genus already has. The main difference here with the above described Gibrat's law -- which also involves proportional growth -- is that in the Yule process the growth is not distributed evenly across the system. There is an additional selecting process that over time gives more to those that already have, thus according a disproportionate advantage to anyone who gets even the slightest advantage by chance from the offset -- which is why this is often referred to as a *rich-get-richer* process when applied to economics. In fact, the Yule process having been recognised more or less independently within a number of disciplines over the years, it has come to be known by a plethora of different appellations, sometimes hiding the fact that they describe the same underlying mechanism. The term *feedback loop* is derived from acoustics, describing the bothering situation when the sound from an on-stage monitor feeds into the microphones and back to the monitors, and so on, thus generating a sound so strong that it

Preferential attachment: [@mitzenmacher2004], @yule1925, @zipf1949, @simon1955 calls it "the Gibrat principle", @mandelbrot1997, short summary and connection to fractal geometry in @mandelbrot1982, pp. 341-8, @barabási1999,

Yule process, Matthew effect, rich-gets-richer, "80-20 rule".

Other processes: phase transitions/critical phenomena - self-organised criticality, punctuated equilibrium. Random walks. Optimisation.

Multiplicative processes: power laws vs. log-normal, long debates [e.g. @harrison1981, @sheridan2018].

Power laws are not consciously made, they emerge spontaneously: is power-law word frequency a good proxy for language (possibly off topic)

-   Notes on terminology: Power law, Pareto and Zipf, @newman2005, pp. 327 and 350. Lack of consensus on notation, also for scaling parameter. Difference between PDF and CDF, continuous and discrete.

-   Upper and lower bounds. Other more complex models: stretched exponential, power law with exponential cutoff, parabolic fractal. Combination log-normal + exponential (i.e. normal + exponential + exponential) [@reed2001; @reed2002]

-   A law (distribution) is not a law (of nature), see @grove2011 for review of the long-lasting confusion in archaeology (e.g. @hodder1979), also "rank-size rule"

## Fitting heavy-tailed distributions in archaeology {#distfit-archaeo}

-   Lit. use @strawinska-zanko2018, @crabtree2017, @maschner2003, @grove2011 ++

-   Zipf law and Settlement Scaling theory, @bettencourt2021, @gomez-lievano2012, @lobo2020. Connection with Central Place Theory, e.g. @müller-scheeßel2007, @chen2011. Why I'm not doing settlement scaling in this study.

-   Not fitting distributions in archaeology, just assuming they are heavy-tailed, or avoiding the question: ex. @brink2013 (could include lots more!)

END chapter
